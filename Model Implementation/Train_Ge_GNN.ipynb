{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fab949d",
        "outputId": "48cfd604-16a5-48cb-d050-c098679fab12"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E6v6fcrjdMV1",
        "outputId": "44e1bc3b-548c-4c72-bb09-8e2a4bb2e6a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Collecting torch==2.4.1+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.4.1%2Bcu124-cp312-cp312-linux_x86_64.whl (797.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m659.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.1+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.19.1%2Bcu124-cp312-cp312-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m167.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.4.1+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.4.1%2Bcu124-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu124) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu124) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu124) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu124) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu124) (2025.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu124) (75.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.99 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (24.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.99 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.4/883.4 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.99 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m931.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.2.65 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.2.65-py3-none-manylinux2014_x86_64.whl (363.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.0.44 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.0.44-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.119 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.119-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.0.99 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.0.99-py3-none-manylinux2014_x86_64.whl (128.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.0.142 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.0.142-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.99 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.99 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.1+cu124) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.1+cu124) (12.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.1+cu124) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.1+cu124) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cpu\n",
            "    Uninstalling torch-2.8.0+cpu:\n",
            "      Successfully uninstalled torch-2.8.0+cpu\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cpu\n",
            "    Uninstalling torchvision-0.23.0+cpu:\n",
            "      Successfully uninstalled torchvision-0.23.0+cpu\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cpu\n",
            "    Uninstalling torchaudio-2.8.0+cpu:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cpu\n",
            "Successfully installed nvidia-cublas-cu12-12.4.2.65 nvidia-cuda-cupti-cu12-12.4.99 nvidia-cuda-nvrtc-cu12-12.4.99 nvidia-cuda-runtime-cu12-12.4.99 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.0.44 nvidia-curand-cu12-10.3.5.119 nvidia-cusolver-cu12-11.6.0.99 nvidia-cusparse-cu12-12.3.0.142 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.4.99 torch-2.4.1+cu124 torchaudio-2.4.1+cu124 torchvision-0.19.1+cu124 triton-3.0.0\n",
            "2.4.1+cu124\n",
            "Looking in links: https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/torch-2.4/cu124/dgl-2.4.0%2Bcu124-cp312-cp312-manylinux1_x86_64.whl (347.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.12/dist-packages (from dgl) (3.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from dgl) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dgl) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from dgl) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from dgl) (4.67.1)\n",
            "Collecting torch<=2.4.0 (from dgl)\n",
            "  Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (2025.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (75.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (2.20.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.4.99)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\n",
            "Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m664.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m994.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m156.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, dgl\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.99\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.99:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.99\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.0.142\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.0.142:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.0.142\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.119\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.119:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.119\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.0.44\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.0.44:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.0.44\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.99\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.99:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.99\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.99\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.99:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.99\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.99\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.99:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.99\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.2.65\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.2.65:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.2.65\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.0.99\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.0.99:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.0.99\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1+cu124\n",
            "    Uninstalling torch-2.4.1+cu124:\n",
            "      Successfully uninstalled torch-2.4.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.19.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dgl-2.4.0+cu124 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "871bd7f3cf9e447686642e8ee3f2d57d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scipy scikit-learn pyyaml\n",
        "!nvidia-smi\n",
        "!pip install torch==2.4.1+cu124 torchvision==0.19.1+cu124 torchaudio==2.4.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n",
        "import dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0qNsM2cdRaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311858d2-b72c-4d65-f174-e91db3247cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Collecting patsy>=0.5.1 (from category_encoders)\n",
            "  Downloading patsy-1.0.2-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
            "Collecting statsmodels>=0.9.0 (from category_encoders)\n",
            "  Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading patsy-1.0.2-py2.py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.3/233.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patsy, statsmodels, category_encoders\n",
            "Successfully installed category_encoders-2.9.0 patsy-1.0.2 statsmodels-0.14.5\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/GE-GNN/MY_work/DataSets/Sports_and_Outdoors.json'\n",
        "object_count = 0\n",
        "try:\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Assuming each line is a valid JSON object\n",
        "            try:\n",
        "                json.loads(line)\n",
        "                object_count += 1\n",
        "            except json.JSONDecodeError:\n",
        "                # Handle lines that are not valid JSON if necessary\n",
        "                pass\n",
        "    print(f\"The file contains {object_count} objects.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVpzdriB0JWu",
        "outputId": "5f1b5fc3-85a9-444c-8873-af47377c06b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file contains 3013256 objects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQfQraB2NpGz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "35318260-761b-48a4-a3d7-0407b114b82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Filtered dataset saved: /content/drive/MyDrive/GE-GNN/MY_work/DataSets/filtered_sports_and_outdoors_100k_fraud_20k_nonfraud_80k.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Low-dimensional features generated:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   overall  helpful_votes  unhelpful_votes  helpful_unhelpful_ratio  day_gap  \\\n",
              "0        1              0                1                      0.0      0.0   \n",
              "1        3              0                0                      0.0      0.0   \n",
              "2        5              1                1                      0.5      0.0   \n",
              "3        5              0                0                      0.0      0.0   \n",
              "4        3              1                1                      0.5      0.0   \n",
              "\n",
              "   same_day_indicator  review_word_count  sentiment_score  \\\n",
              "0                   1                 22          -0.4019   \n",
              "1                   1                 51           0.9224   \n",
              "2                   1                 20           0.9652   \n",
              "3                   1                 26           0.0000   \n",
              "4                   1                 49           0.6297   \n",
              "\n",
              "   total_helpful_votes  total_unhelpful_votes  ...  review_text_svd_3  \\\n",
              "0                    0                      1  ...          -0.021572   \n",
              "1                    0                      0  ...           0.001261   \n",
              "2                    1                      1  ...           0.116697   \n",
              "3                    0                      0  ...           0.043062   \n",
              "4                    1                      1  ...          -0.064168   \n",
              "\n",
              "   review_text_svd_4  review_text_svd_5  review_text_svd_6  summary_svd_0  \\\n",
              "0          -0.097558          -0.007729           0.071458       0.029516   \n",
              "1          -0.042690           0.090665           0.033525       0.055534   \n",
              "2          -0.105428          -0.147906          -0.059715       0.182732   \n",
              "3           0.145209          -0.028022          -0.047291       0.538921   \n",
              "4          -0.073333          -0.029173           0.005126       0.004431   \n",
              "\n",
              "   summary_svd_1  summary_svd_2  reviewerID_encoded  asin_encoded  \\\n",
              "0       0.215615      -0.034107                   1             2   \n",
              "1       0.143786       0.193735                   1             2   \n",
              "2       0.229841      -0.067016                   1             1   \n",
              "3      -0.118240       0.096294                   1             5   \n",
              "4       0.003045       0.000511                   1             1   \n",
              "\n",
              "   reviewerName_encoded  \n",
              "0                     2  \n",
              "1                    12  \n",
              "2                    33  \n",
              "3                     1  \n",
              "4                     1  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e412de4-1c56-4c0c-86cf-7d162ec6989c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>unhelpful_votes</th>\n",
              "      <th>helpful_unhelpful_ratio</th>\n",
              "      <th>day_gap</th>\n",
              "      <th>same_day_indicator</th>\n",
              "      <th>review_word_count</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>total_helpful_votes</th>\n",
              "      <th>total_unhelpful_votes</th>\n",
              "      <th>...</th>\n",
              "      <th>review_text_svd_3</th>\n",
              "      <th>review_text_svd_4</th>\n",
              "      <th>review_text_svd_5</th>\n",
              "      <th>review_text_svd_6</th>\n",
              "      <th>summary_svd_0</th>\n",
              "      <th>summary_svd_1</th>\n",
              "      <th>summary_svd_2</th>\n",
              "      <th>reviewerID_encoded</th>\n",
              "      <th>asin_encoded</th>\n",
              "      <th>reviewerName_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>-0.4019</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.021572</td>\n",
              "      <td>-0.097558</td>\n",
              "      <td>-0.007729</td>\n",
              "      <td>0.071458</td>\n",
              "      <td>0.029516</td>\n",
              "      <td>0.215615</td>\n",
              "      <td>-0.034107</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "      <td>0.9224</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001261</td>\n",
              "      <td>-0.042690</td>\n",
              "      <td>0.090665</td>\n",
              "      <td>0.033525</td>\n",
              "      <td>0.055534</td>\n",
              "      <td>0.143786</td>\n",
              "      <td>0.193735</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0.9652</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.116697</td>\n",
              "      <td>-0.105428</td>\n",
              "      <td>-0.147906</td>\n",
              "      <td>-0.059715</td>\n",
              "      <td>0.182732</td>\n",
              "      <td>0.229841</td>\n",
              "      <td>-0.067016</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.043062</td>\n",
              "      <td>0.145209</td>\n",
              "      <td>-0.028022</td>\n",
              "      <td>-0.047291</td>\n",
              "      <td>0.538921</td>\n",
              "      <td>-0.118240</td>\n",
              "      <td>0.096294</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>49</td>\n",
              "      <td>0.6297</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064168</td>\n",
              "      <td>-0.073333</td>\n",
              "      <td>-0.029173</td>\n",
              "      <td>0.005126</td>\n",
              "      <td>0.004431</td>\n",
              "      <td>0.003045</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e412de4-1c56-4c0c-86cf-7d162ec6989c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5e412de4-1c56-4c0c-86cf-7d162ec6989c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5e412de4-1c56-4c0c-86cf-7d162ec6989c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Features: 24\n",
            "\n",
            "✅ Block-1 completed — No label leakage now!\n",
            "\n",
            "✅ Saving encoders used during training...\n",
            "✅ Encoders saved to: /content/drive/MyDrive/GE-GNN/encoders\n",
            "✅ These must be loaded during inference — do NOT re-fit encoders!\n"
          ]
        }
      ],
      "source": [
        "# Block 1: Data Loading, Filtering, and Low-Dimensional Feature Engineering\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import category_encoders as ce\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import warnings\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the initial dataset\n",
        "file_path = '/content/drive/MyDrive/GE-GNN/MY_work/DataSets/Sports_and_Outdoors.json'\n",
        "df = pd.read_json(file_path, lines=True)\n",
        "\n",
        "# Filter dataset: specified samples per class\n",
        "df_class_0 = df[df['class'] == 0]\n",
        "df_class_1 = df[df['class'] == 1]\n",
        "\n",
        "random_state = 42\n",
        "n_samples_class_0 = 20000  # Frauds\n",
        "n_samples_class_1 = 80000  # Non-frauds\n",
        "\n",
        "df_class_0_sampled = df_class_0.sample(n=min(len(df_class_0), n_samples_class_0), random_state=random_state, replace=False)\n",
        "df_class_1_sampled = df_class_1.sample(n=min(len(df_class_1), n_samples_class_1), random_state=random_state, replace=False)\n",
        "\n",
        "\n",
        "df_filtered_csv = pd.concat([df_class_0_sampled, df_class_1_sampled]).reset_index(drop=True)\n",
        "\n",
        "# Save filtered dataset\n",
        "output_file_path_csv = '/content/drive/MyDrive/GE-GNN/MY_work/DataSets/filtered_sports_and_outdoors_100k_fraud_20k_nonfraud_80k.csv'\n",
        "df_filtered_csv.to_csv(output_file_path_csv, index=False)\n",
        "print(f\"Filtered dataset saved: {output_file_path_csv}\")\n",
        "\n",
        "# ---------------------- Feature Engineering ----------------------\n",
        "\n",
        "# Helpful votes\n",
        "df_filtered_csv['helpful_votes'] = df_filtered_csv['helpful'].apply(lambda x: x[0])\n",
        "df_filtered_csv['unhelpful_votes'] = df_filtered_csv['helpful'].apply(lambda x: x[1])\n",
        "df_filtered_csv['total_votes'] = df_filtered_csv['helpful_votes'] + df_filtered_csv['unhelpful_votes']\n",
        "\n",
        "df_filtered_csv['helpful_unhelpful_ratio'] = df_filtered_csv.apply(\n",
        "    lambda row: row['helpful_votes'] / row['total_votes'] if row['total_votes'] != 0 else 0,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Review time gap features\n",
        "df_filtered_csv['reviewTime_dt'] = pd.to_datetime(df_filtered_csv['reviewTime'], format='%m %d, %Y')\n",
        "df_filtered_csv_sorted = df_filtered_csv.sort_values(by=['reviewerID', 'reviewTime_dt'])\n",
        "df_filtered_csv_sorted['prev_reviewTime_dt'] = df_filtered_csv_sorted.groupby('reviewerID')['reviewTime_dt'].shift(1)\n",
        "df_filtered_csv_sorted['day_gap'] = (df_filtered_csv_sorted['reviewTime_dt'] - df_filtered_csv_sorted['prev_reviewTime_dt']).dt.days.fillna(0)\n",
        "\n",
        "df_filtered_csv = df_filtered_csv.loc[df_filtered_csv_sorted.index].copy()\n",
        "df_filtered_csv['day_gap'] = df_filtered_csv_sorted['day_gap']\n",
        "df_filtered_csv['same_day_indicator'] = (df_filtered_csv['day_gap'] == 0).astype(int)\n",
        "\n",
        "df_filtered_csv['total_helpful_votes'] = df_filtered_csv.groupby('reviewerID')['helpful_votes'].transform('sum')\n",
        "df_filtered_csv['total_unhelpful_votes'] = df_filtered_csv.groupby('reviewerID')['unhelpful_votes'].transform('sum')\n",
        "\n",
        "df_filtered_csv['review_word_count'] = df_filtered_csv['reviewText'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Sentiment scores\n",
        "try:\n",
        "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
        "except LookupError:\n",
        "    nltk.download('vader_lexicon')\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "df_filtered_csv['sentiment_score'] = df_filtered_csv['reviewText'].apply(lambda x: analyzer.polarity_scores(str(x))['compound'])\n",
        "\n",
        "df_filtered_csv['reviewText'] = df_filtered_csv['reviewText'].fillna('')\n",
        "df_filtered_csv['summary'] = df_filtered_csv['summary'].fillna('')\n",
        "df_filtered_csv['reviewerName'] = df_filtered_csv['reviewerName'].fillna('')\n",
        "\n",
        "# TF-IDF + SVD for review text\n",
        "tfidf_review_text = TfidfVectorizer(max_features=5000)\n",
        "review_text_embeddings = tfidf_review_text.fit_transform(df_filtered_csv['reviewText'])\n",
        "svd_review_text = TruncatedSVD(n_components=7, random_state=42)\n",
        "review_text_svd = svd_review_text.fit_transform(review_text_embeddings)\n",
        "review_text_svd_df = pd.DataFrame(review_text_svd, columns=[f'review_text_svd_{i}' for i in range(review_text_svd.shape[1])])\n",
        "\n",
        "# TF-IDF + SVD for summary\n",
        "tfidf_summary = TfidfVectorizer(max_features=2000)\n",
        "summary_embeddings = tfidf_summary.fit_transform(df_filtered_csv['summary'])\n",
        "svd_summary = TruncatedSVD(n_components=3, random_state=42)\n",
        "summary_svd = svd_summary.fit_transform(summary_embeddings)\n",
        "summary_svd_df = pd.DataFrame(summary_svd, columns=[f'summary_svd_{i}' for i in range(summary_svd.shape[1])])\n",
        "\n",
        "# ✅ Frequency Encoding (NO leakage)\n",
        "freq_encoder_reviewerID = ce.CountEncoder(cols=['reviewerID'])\n",
        "reviewerID_encoded = freq_encoder_reviewerID.fit_transform(df_filtered_csv['reviewerID'])\n",
        "reviewerID_encoded.columns = ['reviewerID_encoded']\n",
        "\n",
        "freq_encoder_asin = ce.CountEncoder(cols=['asin'])\n",
        "asin_encoded = freq_encoder_asin.fit_transform(df_filtered_csv['asin'])\n",
        "asin_encoded.columns = ['asin_encoded']\n",
        "\n",
        "freq_encoder_reviewerName = ce.CountEncoder(cols=['reviewerName'])\n",
        "reviewerName_encoded = freq_encoder_reviewerName.fit_transform(df_filtered_csv['reviewerName'])\n",
        "reviewerName_encoded.columns = ['reviewerName_encoded']\n",
        "\n",
        "numerical_features_selected = df_filtered_csv[['overall', 'helpful_votes', 'unhelpful_votes',\n",
        "                                               'helpful_unhelpful_ratio', 'day_gap', 'same_day_indicator',\n",
        "                                               'review_word_count', 'sentiment_score',\n",
        "                                               'total_helpful_votes', 'total_unhelpful_votes', 'total_votes']]\n",
        "\n",
        "# Combine all low-dim features\n",
        "engineered_features_df_low_dim = pd.concat([\n",
        "    numerical_features_selected.reset_index(drop=True),\n",
        "    review_text_svd_df.reset_index(drop=True),\n",
        "    summary_svd_df.reset_index(drop=True),\n",
        "    reviewerID_encoded.reset_index(drop=True),\n",
        "    asin_encoded.reset_index(drop=True),\n",
        "    reviewerName_encoded.reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "print(\"\\nLow-dimensional features generated:\")\n",
        "display(engineered_features_df_low_dim.head())\n",
        "print(f\"Total Features: {engineered_features_df_low_dim.shape[1]}\")\n",
        "print(\"\\n✅ Block-1 completed — No label leakage now!\")\n",
        "\n",
        "# ================================================================\n",
        "# ✅ SAVE ALL ENCODERS FOR FUTURE INFERENCE (VERY IMPORTANT)\n",
        "# ================================================================\n",
        "\n",
        "ENCODER_DIR = \"/content/drive/MyDrive/GE-GNN/encoders\"\n",
        "os.makedirs(ENCODER_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n✅ Saving encoders used during training...\")\n",
        "\n",
        "# TF-IDF + SVD encoders\n",
        "with open(f\"{ENCODER_DIR}/tfidf_review.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf_review_text, f)\n",
        "\n",
        "with open(f\"{ENCODER_DIR}/svd_review.pkl\", \"wb\") as f:\n",
        "    pickle.dump(svd_review_text, f)\n",
        "\n",
        "with open(f\"{ENCODER_DIR}/tfidf_summary.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf_summary, f)\n",
        "\n",
        "with open(f\"{ENCODER_DIR}/svd_summary.pkl\", \"wb\") as f:\n",
        "    pickle.dump(svd_summary, f)\n",
        "\n",
        "# Frequency Encoders\n",
        "with open(f\"{ENCODER_DIR}/freq_reviewerID.pkl\", \"wb\") as f:\n",
        "    pickle.dump(freq_encoder_reviewerID, f)\n",
        "\n",
        "with open(f\"{ENCODER_DIR}/freq_asin.pkl\", \"wb\") as f:\n",
        "    pickle.dump(freq_encoder_asin, f)\n",
        "\n",
        "with open(f\"{ENCODER_DIR}/freq_reviewerName.pkl\", \"wb\") as f:\n",
        "    pickle.dump(freq_encoder_reviewerName, f)\n",
        "\n",
        "print(\"✅ Encoders saved to:\", ENCODER_DIR)\n",
        "print(\"✅ These must be loaded during inference — do NOT re-fit encoders!\")\n",
        "# ================================================================"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is now empty and can be safely deleted."
      ],
      "metadata": {
        "id": "uS5w9AiE9f2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlrsLBJVdkxM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure cell EQfQraB2NpGz (Data Loading, Filtering, and Low-Dimensional Feature Engineering)\n",
        "# has been executed successfully before running this cell to define\n",
        "# df_filtered_csv and engineered_features_df_low_dim.\n",
        "\n",
        "# Assuming df_filtered_csv and engineered_features_df_low_dim are available from previous steps.\n",
        "# If not, reload them:\n",
        "# file_path = '/content/drive/MyDrive/GE-GNN/MY_work/DataSets/filtered_sports_and_outdoors.csv'\n",
        "# df_filtered_csv = pd.read_csv(file_path)\n",
        "# # Reapply feature engineering steps if needed to get engineered_features_df_low_dim\n",
        "# from sklearn.decomposition import TruncatedSVD\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# import category_encoders as ce\n",
        "# ... (reapply feature engineering code)\n",
        "\n",
        "# 1. Extract features and labels\n",
        "features = engineered_features_df_low_dim.values.astype(np.float32)\n",
        "labels = df_filtered_csv['class'].values.astype(np.int64)\n",
        "\n",
        "# Reshape labels to be a 1xN array similar to Amazon.mat\n",
        "labels = labels.reshape(1, -1)\n",
        "\n",
        "# 2. Create mappings for reviewerID, asin, and review index\n",
        "# The review index is simply the DataFrame index after filtering and sampling\n",
        "review_index_map = {original_idx: new_idx for new_idx, original_idx in enumerate(df_filtered_csv.index)}\n",
        "num_nodes = len(df_filtered_csv) # Number of nodes is the number of reviews\n",
        "\n",
        "# 3. Initialize lists to store edge indices for each relationship type\n",
        "upu_src = []\n",
        "upu_dst = []\n",
        "usu_src = []\n",
        "usu_dst = []\n",
        "uvu_src = []\n",
        "uvu_dst = []\n",
        "\n",
        "# Group reviews by product (asin) to find U-P-U relationships\n",
        "for asin, group in df_filtered_csv.groupby('asin'):\n",
        "    review_indices = group.index.tolist()\n",
        "    # Create edges between all pairs of reviews for the same product\n",
        "    for i in range(len(review_indices)):\n",
        "        for j in range(len(review_indices)):\n",
        "            if i != j:\n",
        "                # Use the new node indices from the review_index_map\n",
        "                upu_src.append(review_index_map[review_indices[i]])\n",
        "                upu_dst.append(review_index_map[review_indices[j]])\n",
        "\n",
        "# Group reviews by reviewer (reviewerID) to find U-S-U and U-V-U relationships\n",
        "for reviewer_id, group in df_filtered_csv.groupby('reviewerID'):\n",
        "    review_indices = group.index.tolist()\n",
        "    # Iterate through pairs of reviews by the same reviewer\n",
        "    for i in range(len(review_indices)):\n",
        "        for j in range(len(review_indices)):\n",
        "            if i != j:\n",
        "                review1_idx = review_indices[i]\n",
        "                review2_idx = review_indices[j]\n",
        "\n",
        "                # Check for U-S-U relationship (same sentiment - simplifying to same overall rating)\n",
        "                if df_filtered_csv.loc[review1_idx, 'overall'] == df_filtered_csv.loc[review2_idx, 'overall']:\n",
        "                    usu_src.append(review_index_map[review1_idx])\n",
        "                    usu_dst.append(review_index_map[review2_idx])\n",
        "\n",
        "                # Check for U-V-U relationship (similar review length - simplifying to within a threshold, e.g., 10 words difference)\n",
        "                if abs(df_filtered_csv.loc[review1_idx, 'review_word_count'] - df_filtered_csv.loc[review2_idx, 'review_word_count']) <= 10:\n",
        "                    uvu_src.append(review_index_map[review1_idx])\n",
        "                    uvu_dst.append(review_index_map[review2_idx])\n",
        "\n",
        "\n",
        "# 4. Create sparse adjacency matrices for each relationship type\n",
        "upu_adj_matrix = sp.coo_matrix((np.ones(len(upu_src), dtype=np.float32), (upu_src, upu_dst)), shape=(num_nodes, num_nodes))\n",
        "usu_adj_matrix = sp.coo_matrix((np.ones(len(usu_src), dtype=np.float32), (usu_src, usu_dst)), shape=(num_nodes, num_nodes))\n",
        "uvu_adj_matrix = sp.coo_matrix((np.ones(len(uvu_src), dtype=np.float32), (uvu_src, uvu_dst)), shape=(num_nodes, num_nodes))\n",
        "\n",
        "# 5. Define the path where the output .mat file will be saved.\n",
        "output_mat_file_path_hetero = '/content/drive/MyDrive/GE-GNN/Mat_Dgl_files/amazon_like_sports_outdoors_hetero_20kf_pavan.mat'\n",
        "\n",
        "# 6. Create a dictionary similar to Amazon.mat with heterogeneous graph structure\n",
        "amazon_like_data_hetero = {\n",
        "    '__header__': b'Custom generated MAT-file (Heterogeneous Graph)',\n",
        "    '__version__': '1.0',\n",
        "    '__globals__': [],\n",
        "    'features': features,\n",
        "    'label': labels,\n",
        "    'net_upu': upu_adj_matrix, # U-P-U relationship\n",
        "    'net_usu': usu_adj_matrix, # U-S-U relationship\n",
        "    'net_uvu': uvu_adj_matrix  # U-V-U relationship\n",
        "}\n",
        "\n",
        "# 7. Save the dictionary to a .mat file\n",
        "scipy.io.savemat(output_mat_file_path_hetero, amazon_like_data_hetero)\n",
        "\n",
        "# 8. Print confirmation and shapes\n",
        "print(f\"Processed heterogeneous data saved to {output_mat_file_path_hetero}\")\n",
        "print(f\"Shape of features matrix: {amazon_like_data_hetero['features'].shape}\")\n",
        "print(f\"Shape of label matrix: {amazon_like_data_hetero['label'].shape}\")\n",
        "print(f\"Shape of net_upu adjacency matrix: {amazon_like_data_hetero['net_upu'].shape}\")\n",
        "print(f\"Shape of net_usu adjacency matrix: {amazon_like_data_hetero['net_usu'].shape}\")\n",
        "print(f\"Shape of net_uvu adjacency matrix: {amazon_like_data_hetero['net_uvu'].shape}\")\n",
        "print(f\"Number of net_upu edges: {amazon_like_data_hetero['net_upu'].nnz}\")\n",
        "print(f\"Number of net_usu edges: {amazon_like_data_hetero['net_usu'].nnz}\")\n",
        "print(f\"Number of net_uvu edges: {amazon_like_data_hetero['net_uvu'].nnz}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRfnGnH4do4h"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import dgl\n",
        "import scipy.io as scio\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the path to the generated heterogeneous .mat file and the output DGL graph file\n",
        "mat_file_path_hetero = '/content/drive/MyDrive/GE-GNN/Mat_Dgl_files/amazon_like_sports_outdoors_hetero_20kf_pavan.mat'\n",
        "output_dgl_path_hetero = '/content/drive/MyDrive/GE-GNN/Mat_Dgl_files/sports_outdoors_hetero_pavan_20k_F.dgl'\n",
        "\n",
        "# Load the data from the generated .mat file\n",
        "try:\n",
        "    mat_data_hetero = scio.loadmat(mat_file_path_hetero)\n",
        "    print(\"Generated heterogeneous .mat file loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The generated .mat file was not found at {mat_file_path_hetero}\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the .mat file: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "# Extract features, labels, and adjacency matrices for each relationship type\n",
        "features = torch.from_numpy(mat_data_hetero['features']).float()\n",
        "labels = torch.from_numpy(mat_data_hetero['label'][0]).long() # Assuming labels are in a 1xN array\n",
        "\n",
        "# Get sparse matrices for each relationship\n",
        "upu_adj_matrix = mat_data_hetero['net_upu']\n",
        "usu_adj_matrix = mat_data_hetero['net_usu']\n",
        "uvu_adj_matrix = mat_data_hetero['net_uvu']\n",
        "\n",
        "# Convert sparse matrices to source and destination node lists for DGL\n",
        "upu_src = torch.from_numpy(upu_adj_matrix.row).int()\n",
        "upu_dst = torch.from_numpy(upu_adj_matrix.col).int()\n",
        "\n",
        "usu_src = torch.from_numpy(usu_adj_matrix.row).int()\n",
        "usu_dst = torch.from_numpy(usu_adj_matrix.col).int()\n",
        "\n",
        "uvu_src = torch.from_numpy(uvu_adj_matrix.row).int()\n",
        "uvu_dst = torch.from_numpy(uvu_adj_matrix.col).int()\n",
        "\n",
        "\n",
        "# Define the graph structure for DGL (heterogeneous graph with a single node type 'r' for review)\n",
        "# Node type 'r' (review) and edge types 'p' (product), 's' (sentiment), 'v' (review length)\n",
        "graph_structure_hetero = {\n",
        "    ('r', 'p', 'r'): (upu_src, upu_dst), # Reviews connected if for same Product\n",
        "    ('r', 's', 'r'): (usu_src, usu_dst), # Reviews connected if by same User with same Sentiment\n",
        "    ('r', 'v', 'r'): (uvu_src, uvu_dst)  # Reviews connected if by same User with similar review Length\n",
        "}\n",
        "\n",
        "# The number of nodes should be the number of reviews, which is the number of rows in the features matrix\n",
        "num_nodes = features.shape[0]\n",
        "\n",
        "# Create the DGL heterogeneous graph with the correct number of nodes\n",
        "sports_outdoors_hetero_graph = dgl.heterograph(graph_structure_hetero, num_nodes_dict={'r': num_nodes})\n",
        "\n",
        "# Add features and labels to the nodes\n",
        "sports_outdoors_hetero_graph.nodes['r'].data['feat'] = features\n",
        "sports_outdoors_hetero_graph.nodes['r'].data['label'] = labels\n",
        "\n",
        "# Optional: Add self-loops for each edge type if needed (based on original script)\n",
        "for etype in sports_outdoors_hetero_graph.etypes:\n",
        "    sports_outdoors_hetero_graph.add_self_loop(etype=etype)\n",
        "\n",
        "# Generate dataset partition (train, validation, test masks)\n",
        "index = list(range(num_nodes))\n",
        "dataset_l = num_nodes\n",
        "\n",
        "# Using the same random_state as the original filtering step for reproducibility\n",
        "random_state = 42\n",
        "train_ratio = 0.4 # Example ratios, adjust as needed\n",
        "test_ratio = 0.67 # Example ratios, adjust as needed\n",
        "\n",
        "# Stratified split based on labels\n",
        "train_idx, rest_idx, train_lbs, rest_lbs = train_test_split(\n",
        "    index, labels.numpy(), stratify=labels.numpy(), train_size=train_ratio, random_state=random_state, shuffle=True\n",
        ")\n",
        "valid_idx, test_idx, _, _ = train_test_split(\n",
        "    rest_idx, rest_lbs, stratify=rest_lbs, test_size=test_ratio, random_state=random_state, shuffle=True\n",
        ")\n",
        "\n",
        "# Create masks\n",
        "train_mask = torch.zeros(dataset_l, dtype=torch.bool)\n",
        "train_mask[np.array(train_idx)] = True\n",
        "valid_mask = torch.zeros(dataset_l, dtype=torch.bool)\n",
        "valid_mask[np.array(valid_idx)] = True\n",
        "test_mask = torch.zeros(dataset_l, dtype=torch.bool)\n",
        "test_mask[np.array(test_idx)] = True\n",
        "\n",
        "# Add masks to the graph\n",
        "sports_outdoors_hetero_graph.nodes['r'].data['trn_msk'] = train_mask\n",
        "sports_outdoors_hetero_graph.nodes['r'].data['val_msk'] = valid_mask\n",
        "sports_outdoors_hetero_graph.nodes['r'].data['tst_msk'] = test_mask\n",
        "\n",
        "# Save the DGL graph\n",
        "dgl.save_graphs(output_dgl_path_hetero, sports_outdoors_hetero_graph)\n",
        "\n",
        "print(f\"DGL heterogeneous graph saved to {output_dgl_path_hetero}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ge_gnn_train.py\n",
        "# Full, corrected, ready-to-run script for the GE-GNN pipeline.\n",
        "# Expects a YAML config like the one you provided (dataset, seed, epoch, lr, cuda, ...)\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import yaml\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import dgl\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, recall_score\n",
        "\n",
        "# ----------------------\n",
        "# Utility functions\n",
        "# ----------------------\n",
        "def setup_seed(seed: int):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def normalize_rows(mx: np.ndarray):\n",
        "    # row-normalize dense matrix\n",
        "    rowsum = np.array(mx.sum(1)) + 1e-8\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    return (r_inv[:, None] * mx).astype(np.float32)\n",
        "\n",
        "def conf_gmean(conf):\n",
        "    # conf is 2x2 [ [tn, fp], [fn, tp] ]\n",
        "    tn, fp, fn, tp = conf.ravel()\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    return float(np.sqrt(tpr * tnr))\n",
        "\n",
        "def evaluate(labels, logits, result_path=''):\n",
        "    # logits: torch.Tensor (N, n_class)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n",
        "    preds = logits.argmax(1).detach().cpu().numpy()\n",
        "    if len(result_path) > 0:\n",
        "        np.save(result_path + '_result_preds.npy', preds)\n",
        "        np.save(result_path + '_result_probs.npy', probs)\n",
        "    conf = confusion_matrix(labels, preds)\n",
        "    recall = recall_score(labels, preds, zero_division=0)\n",
        "    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
        "    auc = roc_auc_score(labels, probs) if len(np.unique(labels)) > 1 else 0.5\n",
        "    gmean = conf_gmean(conf)\n",
        "    return f1_macro, auc, gmean, recall\n",
        "\n",
        "# ----------------------\n",
        "# Model components\n",
        "# ----------------------\n",
        "class RelationAware(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.d_linear = nn.Linear(input_dim, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, dst):\n",
        "        src = self.d_linear(src)\n",
        "        dst = self.d_linear(dst)\n",
        "        diff = src - dst\n",
        "        edge_sum = self.tanh(src + dst + diff)\n",
        "        return edge_sum\n",
        "\n",
        "class H_layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, head, relation_aware, etype, dropout, if_sum=False):\n",
        "        super().__init__()\n",
        "        self.etype = etype\n",
        "        self.head = head\n",
        "        self.hd = output_dim\n",
        "        self.if_sum = if_sum\n",
        "        self.atten = nn.Linear(3 * self.hd, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.relation_aware = relation_aware\n",
        "        self.leakyrelu = nn.LeakyReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.w_linear = nn.Linear(input_dim, output_dim * head)\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        # Works with both homograph and heterograph as in your original code (you used g.ndata)\n",
        "        with g.local_scope():\n",
        "            # Ensure 'feat' data is accessible within the local scope for this layer\n",
        "            if 'r' in g.ntypes and 'feat' in g.nodes['r'].data:\n",
        "                g.ndata['feat'] = g.nodes['r'].data['feat']\n",
        "            elif 'feat' in g.ndata:\n",
        "                 g.ndata['feat'] = g.ndata['feat']\n",
        "            else:\n",
        "                 raise RuntimeError(\"Graph missing 'feat' in ndata or nodes['r'].data for H_layer.\")\n",
        "\n",
        "            # apply edge function on the given etype\n",
        "            try:\n",
        "                g.apply_edges(self.sign_edges, etype=self.etype)\n",
        "                edge_sum = g.edges[self.etype].data['edge_sum']\n",
        "            except Exception:\n",
        "                # fallback to global edata (if using homogeneous)\n",
        "                g.apply_edges(self.sign_edges)\n",
        "                edge_sum = g.edata['edge_sum']\n",
        "\n",
        "            h_proj = self.w_linear(h)\n",
        "            g.ndata['h'] = h_proj\n",
        "            try:\n",
        "                g.update_all(self.message, self.reduce, etype=self.etype)\n",
        "            except Exception:\n",
        "                g.update_all(self.message, self.reduce)\n",
        "\n",
        "            out = g.ndata['out']\n",
        "            edge_s = g.ndata['s']\n",
        "            if not self.if_sum:\n",
        "                return edge_s, out, h_proj.view(-1, self.head * self.hd)\n",
        "            else:\n",
        "                return edge_s, out, h_proj.view(-1, self.head, self.hd).sum(-2)\n",
        "\n",
        "    def message(self, edges):\n",
        "        src_f = edges.src['h'].view(-1, self.head, self.hd)\n",
        "        dst_f = edges.dst['h'].view(-1, self.head, self.hd)\n",
        "        edge_s = edges.data['edge_sum'].view(-1, self.head, self.hd)\n",
        "        z = torch.cat([src_f, dst_f, edge_s], dim=-1)  # (E, head, 3*hd)\n",
        "        alpha = self.atten(z)  # (E, head, 1)\n",
        "        alpha = self.leakyrelu(alpha)\n",
        "        return {'atten': alpha, 'sf': src_f, 'edge_s': edge_s}\n",
        "\n",
        "    def reduce(self, nodes):\n",
        "        alpha = nodes.mailbox['atten']  # (N, deg, head, 1)\n",
        "        sf = nodes.mailbox['sf']        # (N, deg, head, hd)\n",
        "        alpha = self.softmax(alpha)\n",
        "        out = torch.sum(alpha * sf, dim=1)  # (N, head, hd)\n",
        "        if not self.if_sum:\n",
        "            out = out.view(-1, self.head * self.hd)\n",
        "            edge_s = torch.mean(nodes.mailbox['edge_s'], dim=1).view(-1, self.head * self.hd)\n",
        "            return {'out': out, 's': edge_s}\n",
        "        else:\n",
        "            out = out.sum(dim=-2)\n",
        "            edge_s = torch.sum(torch.mean(nodes.mailbox['edge_s'], dim=1), dim=-2)\n",
        "            return {'out': out, 's': edge_s}\n",
        "\n",
        "\n",
        "    def sign_edges(self, edges):\n",
        "        src = edges.src['feat']\n",
        "        dst = edges.dst['feat']\n",
        "        edge_sum = self.relation_aware(src, dst)\n",
        "        return {'edge_sum': edge_sum}\n",
        "\n",
        "\n",
        "class Gate(nn.Module):\n",
        "    def __init__(self, head, output_dim, dropout, if_sum=False):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.head = head\n",
        "        if not if_sum:\n",
        "            self.beta = nn.Parameter(torch.empty(size=(2 * self.head * self.output_dim, 1)))\n",
        "            nn.init.xavier_normal_(self.beta.data, gain=1.414)\n",
        "        else:\n",
        "            self.beta = nn.Parameter(torch.empty(size=(2 * self.output_dim, 1)))\n",
        "            nn.init.xavier_normal_(self.beta.data, gain=1.414)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, edge_sum, out, h):\n",
        "        beta = torch.cat([edge_sum, out], dim=1)\n",
        "        gate = self.sigmoid(torch.matmul(beta, self.beta))\n",
        "        final = gate * out + (1 - gate) * h\n",
        "        return final\n",
        "\n",
        "class MultiRelationGE_GNNLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, head, dataset, dropout, if_sum=False):\n",
        "        super().__init__()\n",
        "        # dataset here is the DGLGraph object or wrapper; original code used dataset.etypes\n",
        "        self.relation = copy.deepcopy(dataset.etypes) if hasattr(dataset, 'etypes') else []\n",
        "        if 'homo' in self.relation:\n",
        "            self.relation.remove('homo')\n",
        "        self.n_relation = len(self.relation)\n",
        "        self.if_sum = if_sum\n",
        "        if not self.if_sum:\n",
        "            self.liner = nn.Linear(max(1, self.n_relation) * output_dim * head, output_dim * head)\n",
        "        else:\n",
        "            self.liner = nn.Linear(max(1, self.n_relation) * output_dim, output_dim)\n",
        "        self.relation_aware = RelationAware(input_dim, output_dim * head, dropout)\n",
        "\n",
        "        self.minelayers = nn.ModuleDict()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        for e in (self.relation if len(self.relation) > 0 else ['homo']):\n",
        "            sub = nn.ModuleList()\n",
        "            sub.append(H_layer(input_dim, output_dim, head, self.relation_aware, e, dropout, if_sum))\n",
        "            sub.append(Gate(head, output_dim, dropout, if_sum))\n",
        "            self.minelayers[e] = sub\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        hs = []\n",
        "        for e, modules in self.minelayers.items():\n",
        "            edge_sum1, out1, h1 = modules[0](g, h)\n",
        "            he = modules[1](edge_sum1, out1, h1)\n",
        "            hs.append(he)\n",
        "        x = torch.cat(hs, dim=1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.liner(x)\n",
        "        return x\n",
        "\n",
        "    # Note: The loss method was part of the MultiRelationGE_GNNLayer in the original code,\n",
        "    # but it seems it should be in the main GE_GNN model for the training loop to work correctly.\n",
        "    # If the original code's loss calculation logic is needed, it should be adapted or moved\n",
        "    # to a training loop that utilizes the forward pass and calculates loss based on model outputs and labels.\n",
        "    # For now, I will provide a basic loss calculation based on the model's forward pass\n",
        "    # and available training masks/labels, assuming the required data is in g.ndata or g.nodes['r'].data.\n",
        "\n",
        "    def loss(self, g):\n",
        "        # Compute training loss.\n",
        "        # This loss function structure was copied from the original code's MultiRelationGE_GNNLayer.\n",
        "        # To fully replicate the original training, this logic should likely be adapted or moved\n",
        "        # to a training loop that utilizes the forward pass and calculates loss based on model outputs and labels.\n",
        "        # For now, I will provide a basic loss calculation based on the model's forward pass\n",
        "        # and available training masks/labels, assuming the required data is in g.ndata or g.nodes['r'].data.\n",
        "\n",
        "        # Get training mask and labels, handling heterogeneous graphs\n",
        "        if 'r' in g.ntypes and 'trn_msk' in g.nodes['r'].data: # Corrected mask name\n",
        "             train_mask = g.nodes['r'].data['trn_msk'].bool()\n",
        "             labels = g.nodes['r'].data['label']\n",
        "        elif 'train_mask' in g.ndata:\n",
        "            train_mask = g.ndata['train_mask'].bool()\n",
        "            labels = g.ndata['label']\n",
        "        else:\n",
        "             print(\"Error: Graph missing 'trn_msk' node data for loss calculation.\")\n",
        "             return torch.tensor(0.0, device=g.device)\n",
        "\n",
        "        # Ensure there are training samples\n",
        "        if not train_mask.any():\n",
        "            print(\"Warning: No nodes in the training mask. Skipping loss calculation.\")\n",
        "            return torch.tensor(0.0, device=g.device)\n",
        "\n",
        "        train_label = labels[train_mask]\n",
        "\n",
        "        # Perform forward pass to get logits\n",
        "        logits = self.forward(g)[train_mask]\n",
        "\n",
        "        # Calculate cross-entropy loss\n",
        "        model_loss = F.cross_entropy(logits, train_label)\n",
        "\n",
        "        # Note: Prototype loss from original code is not implemented here as it was in MultiRelationGE_GNNLayer.\n",
        "        # If needed, this method should be updated to reflect the correct loss calculation logic\n",
        "        # based on the original GE-GNN implementation, potentially involving prototype calculations.\n",
        "        # For now, returning only the cross-entropy loss.\n",
        "        # print(\"Warning: Prototype loss calculation is not fully implemented in the loss method.\")\n",
        "\n",
        "        return model_loss\n",
        "\n",
        "\n",
        "class GE_GNN(nn.Module):\n",
        "    def __init__(self, args, g):\n",
        "        super().__init__()\n",
        "        self.n_layer = args.n_layer\n",
        "        # Determine input_dim based on graph features, prioritizing heterogeneous node data\n",
        "        if hasattr(g, 'ntypes') and 'r' in g.ntypes and 'feat' in g.nodes['r'].data: # Check for 'feat' in heterogeneous graph\n",
        "            self.input_dim = g.nodes['r'].data['feat'].shape[1]\n",
        "        elif 'feature' in g.ndata: # Fallback to homogeneous graph 'feature'\n",
        "            self.input_dim = g.ndata['feature'].shape[1]\n",
        "        else:\n",
        "            # Fallback or raise error if features are not found\n",
        "            print(\"Warning: 'feat' not found in nodes['r'].data and 'feature' not found in ndata. Assuming input_dim from config if available.\")\n",
        "            self.input_dim = getattr(args, 'input_dim', 0) # Use config if available, default to 0\n",
        "            if self.input_dim == 0:\n",
        "                 raise RuntimeError(\"Graph does not contain 'feat' node data (heterogeneous) or 'feature' node data (homogeneous) and 'input_dim' not specified in config.\")\n",
        "\n",
        "\n",
        "        self.intra_dim = args.intra_dim\n",
        "        self.n_class = args.n_class\n",
        "        self.gamma1 = args.gamma1\n",
        "        self.gamma2 = args.gamma2\n",
        "        self.mine_layers = nn.ModuleList()\n",
        "\n",
        "        if args.n_layer == 1:\n",
        "            self.mine_layers.append(MultiRelationGE_GNNLayer(self.input_dim, self.n_class, args.head, g, args.dropout, if_sum=True))\n",
        "        else:\n",
        "            self.mine_layers.append(MultiRelationGE_GNNLayer(self.input_dim, self.intra_dim, args.head, g, args.dropout))\n",
        "            for _ in range(1, args.n_layer - 1):\n",
        "                # Input dimension for subsequent layers is the output dimension of the previous layer\n",
        "                prev_layer_output_dim = self.intra_dim * args.head # Assuming MultiRelationGE_GNNLayer outputs concatenated heads\n",
        "                self.mine_layers.append(MultiRelationGE_GNNLayer(prev_layer_output_dim, self.intra_dim, args.head, g, args.dropout))\n",
        "            # Final layer sums the heads\n",
        "            prev_layer_output_dim = self.intra_dim * args.head # Output of the last intermediate layer\n",
        "            self.mine_layers.append(MultiRelationGE_GNNLayer(prev_layer_output_dim, self.n_class, args.head, g, args.dropout, if_sum=True))\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, g):\n",
        "        # Get features from the graph, handling heterogeneous graphs\n",
        "        if 'r' in g.ntypes and 'feat' in g.nodes['r'].data: # Prioritize heterogeneous graph 'feat'\n",
        "             feats = g.nodes['r'].data['feat'].float()\n",
        "        elif 'feature' in g.ndata: # Fallback to homogeneous graph 'feature'\n",
        "            feats = g.ndata['feature'].float()\n",
        "        else:\n",
        "            raise RuntimeError(\"Graph missing 'feat' in nodes['r'].data or 'feature' in ndata for forward pass.\")\n",
        "\n",
        "\n",
        "        h = feats # Start with initial features as input to the first layer\n",
        "        for i, layer in enumerate(self.mine_layers):\n",
        "            h = layer(g, h)\n",
        "            if i < len(self.mine_layers) - 1: # Apply ReLU and Dropout for intermediate layers\n",
        "                h = self.relu(h)\n",
        "                h = self.dropout(h)\n",
        "        return h\n",
        "\n",
        "    def loss(self, g):\n",
        "        # Compute training loss.\n",
        "        # This loss function structure was copied from the original code's MultiRelationGE_GNNLayer.\n",
        "        # To fully replicate the original training, this logic should likely be adapted or moved\n",
        "        # to a training loop that utilizes the forward pass and calculates loss based on model outputs and labels.\n",
        "        # For now, I will provide a basic loss calculation based on the model's forward pass\n",
        "        # and available training masks/labels, assuming the required data is in g.ndata or g.nodes['r'].data.\n",
        "\n",
        "        # Get training mask and labels, handling heterogeneous graphs\n",
        "        if 'r' in g.ntypes and 'trn_msk' in g.nodes['r'].data: # Corrected mask name\n",
        "             train_mask = g.nodes['r'].data['trn_msk'].bool()\n",
        "             labels = g.nodes['r'].data['label']\n",
        "        elif 'train_mask' in g.ndata:\n",
        "            train_mask = g.ndata['train_mask'].bool()\n",
        "            labels = g.ndata['label']\n",
        "        else:\n",
        "             print(\"Error: Graph missing 'trn_msk' node data for loss calculation.\")\n",
        "             return torch.tensor(0.0, device=g.device)\n",
        "\n",
        "        # Ensure there are training samples\n",
        "        if not train_mask.any():\n",
        "            print(\"Warning: No nodes in the training mask. Skipping loss calculation.\")\n",
        "            return torch.tensor(0.0, device=g.device)\n",
        "\n",
        "        train_label = labels[train_mask]\n",
        "\n",
        "        # Perform forward pass to get logits\n",
        "        logits = self.forward(g)[train_mask]\n",
        "\n",
        "        # Calculate cross-entropy loss\n",
        "        model_loss = F.cross_entropy(logits, train_label)\n",
        "\n",
        "        # Note: Prototype loss from original code is not implemented here as it was in MultiRelationGE_GNNLayer.\n",
        "        # If needed, this method should be updated to reflect the correct loss calculation logic\n",
        "        # based on the original GE-GNN implementation, potentially involving prototype calculations.\n",
        "        # For now, returning only the cross-entropy loss.\n",
        "        # print(\"Warning: Prototype loss calculation is not fully implemented in the loss method.\")\n",
        "\n",
        "        return model_loss\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Training script\n",
        "# ----------------------\n",
        "def load_config(yaml_path):\n",
        "    with open(yaml_path, 'r') as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    return cfg\n",
        "\n",
        "def train_graph(args, g, model_path_base, dataset_name):\n",
        "    \"\"\"Trains the GE-GNN model on a single DGL graph.\"\"\"\n",
        "    # build model\n",
        "    model = GE_GNN(args, g).to(args.device)\n",
        "\n",
        "    # optimizer and scheduler\n",
        "    optimizer = optim.Adam(model.parameters(), lr=float(args.lr), weight_decay=float(args.weight_decay))\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=20, verbose=True)\n",
        "\n",
        "    best_auc = -1.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    # gradient clip\n",
        "    max_grad_norm = 5.0\n",
        "\n",
        "    print(f\"Start training model for {dataset_name}...\")\n",
        "    for e in range(int(args.epoch)):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # Calculate loss using the model's loss method\n",
        "        loss = model.loss(g)\n",
        "        loss.backward()\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Ensure valid_mask exists in g.ndata or g.nodes['r'].data\n",
        "            if 'r' in g.ntypes and 'val_msk' in g.nodes['r'].data: # Corrected mask name\n",
        "                 valid_mask = g.nodes['r'].data['val_msk'].bool()\n",
        "                 valid_labels = g.nodes['r'].data['label'][valid_mask].cpu().numpy()\n",
        "                 logits = model(g)[valid_mask]\n",
        "            elif 'valid_mask' in g.ndata:\n",
        "                valid_mask = g.ndata['valid_mask'].bool()\n",
        "                valid_labels = g.ndata['label'][valid_mask].cpu().numpy()\n",
        "                logits = model(g)[valid_mask]\n",
        "            else:\n",
        "                 print(\"Warning: 'val_msk' not found in graph. Skipping validation.\")\n",
        "                 continue # Skip validation if mask is missing\n",
        "\n",
        "            if len(valid_labels) == 0:\n",
        "                 print(\"Warning: No validation samples found. Skipping validation metrics.\")\n",
        "                 auc = 0.0 # Assign a default low AUC\n",
        "                 f1_macro, gmean, recall = 0.0, 0.0, 0.0\n",
        "            else:\n",
        "                f1_macro, auc, gmean, recall = evaluate(valid_labels, logits)\n",
        "\n",
        "        if args.log:\n",
        "            print(f\"Dataset: {dataset_name}, Epoch: {e}: Loss:{loss.item():.4f}, Valid AUC:{auc:.6f}, Recall:{recall:.4f}, F1:{f1_macro:.4f}, G-Mean:{gmean:.4f}\")\n",
        "\n",
        "        # scheduler step by validation AUC\n",
        "        scheduler.step(auc)\n",
        "\n",
        "        # early save\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_epoch = e\n",
        "            # Save model with dataset name in filename\n",
        "            current_model_path = os.path.join(args.result_path, f\"20k_fraud_model_head{args.head}.pt\")\n",
        "            torch.save(model.state_dict(), current_model_path)\n",
        "            if args.log:\n",
        "                print(f\"Saved best model for {dataset_name} at epoch {e}, best_auc={best_auc:.6f}\")\n",
        "\n",
        "        # early stopping\n",
        "        # simple patience tracking using early_stop from config\n",
        "        if (e - best_epoch) >= int(args.early_stop):\n",
        "            if args.log:\n",
        "                print(f\"Dataset: {dataset_name}: No improvement for {args.early_stop} epochs. Early stopping at epoch {e}.\")\n",
        "            break\n",
        "\n",
        "    print(f\"End training for {dataset_name}\")\n",
        "\n",
        "    # test\n",
        "    final_model_path = os.path.join(args.result_path, f\"20k_fraud_model_head{args.head}.pt\")\n",
        "    if os.path.exists(final_model_path):\n",
        "        model.load_state_dict(torch.load(final_model_path, map_location=args.device))\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Ensure test_mask exists in g.ndata or g.nodes['r'].data\n",
        "             if 'r' in g.ntypes and 'tst_msk' in g.nodes['r'].data: # Corrected mask name\n",
        "                 test_mask = g.nodes['r'].data['tst_msk'].bool()\n",
        "                 test_labels = g.nodes['r'].data['label'][test_mask].cpu().numpy()\n",
        "                 logits = model(g)[test_mask].cpu()\n",
        "             elif 'test_mask' in g.ndata:\n",
        "                test_mask = g.ndata['test_mask'].bool()\n",
        "                test_labels = g.ndata['label'][test_mask].cpu().numpy()\n",
        "                logits = model(g)[test_mask].cpu()\n",
        "             else:\n",
        "                 print(f\"Warning: 'tst_msk' not found in graph for {dataset_name}. Skipping test evaluation.\")\n",
        "                 return # Skip test if mask is missing\n",
        "\n",
        "             if len(test_labels) == 0:\n",
        "                 print(f\"Warning: No test samples found for {dataset_name}. Skipping test metrics.\")\n",
        "             else:\n",
        "                f1_macro, auc, gmean, recall = evaluate(test_labels, logits, os.path.join(args.result_path, dataset_name))\n",
        "                print(f\"Test Results for {dataset_name}: F1-macro:{f1_macro:.6f}, AUC:{auc:.6f}, G-Mean:{gmean:.6f}, Recall:{recall:.6f}\")\n",
        "    else:\n",
        "        print(f\"No saved model found at {final_model_path}; skipping test for {dataset_name}.\")\n",
        "\n",
        "    return g # Return the processed graph\n",
        "\n",
        "\n",
        "def main(config_path):\n",
        "    \"\"\"Main function to load config and run training.\"\"\"\n",
        "    cfg = load_config(config_path)\n",
        "    args = argparse.Namespace(**cfg)\n",
        "\n",
        "    setup_seed(args.seed)\n",
        "\n",
        "    args.device = torch.device('cuda' if args.cuda and torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # --- Load the specified DGL graph ---\n",
        "    dgl_graph_path = \"/content/drive/MyDrive/GE-GNN/Mat_Dgl_files/sports_outdoors_hetero_pavan_20k_F.dgl\"\n",
        "    try:\n",
        "        graphs, labels_dict = dgl.load_graphs(dgl_graph_path)\n",
        "        g = graphs[0] # Assuming the DGL file contains a list of graphs, and we want the first one\n",
        "        print(f\"Loaded DGL graph from: {dgl_graph_path}\")\n",
        "\n",
        "        # Ensure graph and node data are on the correct device\n",
        "        g = g.to(args.device)\n",
        "        if 'r' in g.ntypes:\n",
        "            for key in g.nodes['r'].data:\n",
        "                g.nodes['r'].data[key] = g.nodes['r'].data[key].to(args.device)\n",
        "        # If it's a homogeneous graph\n",
        "        else:\n",
        "             for key in g.ndata:\n",
        "                g.ndata[key] = g.ndata[key].to(args.device)\n",
        "\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: DGL graph file not found at {dgl_graph_path}\")\n",
        "        sys.exit()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the DGL graph: {e}\")\n",
        "        sys.exit()\n",
        "    # ------------------------------------\n",
        "\n",
        "\n",
        "    # Set dataset_name for output file naming\n",
        "    # The dataset_name variable was previously used in the filename.\n",
        "    # Now we will hardcode \"20k_fraud\" into the filename logic below.\n",
        "    # dataset_name = os.path.splitext(os.path.basename(dgl_graph_path))[0]\n",
        "    dataset_name = \"sports_outdoors_20k_F\" # Set dataset_name explicitly for consistent naming\n",
        "\n",
        "    # Create results directory if it doesn't exist\n",
        "    if not os.path.exists(args.result_path):\n",
        "        os.makedirs(args.result_path)\n",
        "\n",
        "    # Train the model, passing a relevant name for the output file if needed by train_graph\n",
        "    # We will pass a simplified name since the specific \"20k_fraud\" is now hardcoded in train_graph\n",
        "    processed_graph = train_graph(args, g, args.result_path, dataset_name)\n",
        "    return processed_graph\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Change this path if necessary; points to your uploaded YAML file\n",
        "    config_path = \"/content/drive/MyDrive/GE-GNN/config/amazon.yaml\"\n",
        "    # Assign the returned graph to the global variable 'g'\n",
        "    g = main(config_path)"
      ],
      "metadata": {
        "id": "rfaFOoUhIAXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_path = \"/content/drive/MyDrive/GE-GNN/config/amazon.yaml\"\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import dgl\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io as scio\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "import yaml\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, recall_score\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "def setup_seed(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def parse_arg():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dataset', type=str, default='amazon')\n",
        "    args = parser.parse_args()\n",
        "    config_path = './config/'+args.dataset+'.yaml'\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    args = argparse.Namespace(**config)\n",
        "    print('----------------------------------')\n",
        "    print('              args')\n",
        "    print('----------------------------------')\n",
        "    print(f'dataset:\\t{args.dataset}')\n",
        "    print(f'seed:\\t{args.seed}')\n",
        "    print(f'epoch:\\t{args.epoch}')\n",
        "    print(f'early_stop:\\t{args.early_stop}')\n",
        "    print(f'lr:\\t{args.lr}')\n",
        "    print(f'weigth_decay:{args.weight_decay}')\n",
        "    print(f'gamma1:\\t{args.gamma1}')\n",
        "    print(f'gamma2:\\t{args.gamma2}')\n",
        "    print(f'intra_dim:\\t{args.intra_dim}')\n",
        "    print(f'head:\\t{args.head}')\n",
        "    print(f'n_layer:\\t{args.n_layer}')\n",
        "    print(f'dropout:\\t{args.dropout}')\n",
        "    print(f'cuda:\\t{args.cuda}')\n",
        "    print('----------------------------------')\n",
        "    return args\n",
        "# args = parse_arg() # Comment out or remove this line to avoid parsing command line arguments in Colab\n",
        "\n",
        "\n",
        "class EarlyStop():\n",
        "    def __init__(self, early_stop, if_more=True) -> None:\n",
        "        self.best_eval = 0\n",
        "        self.best_epoch = 0\n",
        "        self.if_more = if_more\n",
        "        self.early_stop = early_stop\n",
        "        self.stop_steps = 0\n",
        "\n",
        "    def step(self, current_eval, current_epoch):\n",
        "        do_stop = False\n",
        "        do_store = False\n",
        "        if self.if_more:\n",
        "            if current_eval > self.best_eval:\n",
        "                self.best_eval = current_eval\n",
        "                self.best_epoch = current_epoch\n",
        "                self.stop_steps = 1\n",
        "                do_store = True\n",
        "            else:\n",
        "                self.stop_steps += 1\n",
        "                if self.stop_steps >= self.early_stop:\n",
        "                    do_stop = True\n",
        "        else:\n",
        "            if current_eval < self.best_eval:\n",
        "                self.best_eval = current_eval\n",
        "                self.best_epoch = current_epoch\n",
        "                self.stop_steps = 1\n",
        "                do_store = True\n",
        "            else:\n",
        "                self.stop_steps += 1\n",
        "                if self.stop_steps >= self.early_stop:\n",
        "                    do_stop = True\n",
        "        return do_store, do_stop\n",
        "\n",
        "def conf_gmean(conf):\n",
        "        tn, fp, fn, tp = conf.ravel()\n",
        "        # Avoid division by zero if denominator is zero\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "        return (tpr*tnr)**0.5\n",
        "\n",
        "def prob2pred(prob, threshhold=0.5):\n",
        "    pred = np.zeros_like(prob, dtype=np.int32)\n",
        "    pred[prob >= threshhold] = 1\n",
        "    pred[prob < threshhold] = 0\n",
        "    return pred\n",
        "\n",
        "def evaluate(labels, logits, result_path = ''):\n",
        "    probs = F.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "    preds = logits.argmax(1).cpu().numpy()\n",
        "    if len(result_path)>0:\n",
        "        np.save(result_path+'_result_preds', preds)\n",
        "        np.save(result_path+'_result_probs'\n",
        "                , probs)\n",
        "    conf = confusion_matrix(labels, preds)\n",
        "    recall = recall_score(labels, preds, zero_division=0) # Added zero_division\n",
        "    f1_macro = f1_score(labels, preds, average='macro', zero_division=0) # Added zero_division\n",
        "    auc = roc_auc_score(labels, probs) if len(np.unique(labels)) > 1 else 0.5 # Added check for unique labels\n",
        "    gmean = conf_gmean(conf)\n",
        "    return f1_macro, auc, gmean, recall\n",
        "\n",
        "def hinge_loss(labels, scores):\n",
        "    margin = 1\n",
        "    ls = labels*scores\n",
        "\n",
        "    loss = F.relu(margin-ls)\n",
        "    loss = loss.mean()\n",
        "    return loss\n",
        "\n",
        "def normalize(mx):\n",
        "        \"\"\"\n",
        "                Row-normalize sparse matrix\n",
        "                Code from https://github.com/williamleif/graphsage-simple/\n",
        "        \"\"\"\n",
        "        rowsum = np.array(mx.sum(1)) + 0.01\n",
        "        r_inv = np.power(rowsum, -1).flatten()\n",
        "        r_inv[np.isinf(r_inv)] = 0.\n",
        "        r_mat_inv = sp.diags(r_inv)\n",
        "        mx = r_mat_inv.dot(mx)\n",
        "        return mx\n",
        "class H_layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, head, relation_aware, etype, dropout, if_sum=False):\n",
        "        super().__init__()\n",
        "        self.etype = etype\n",
        "        self.head = head\n",
        "        self.hd = output_dim\n",
        "        self.if_sum = if_sum\n",
        "        self.atten = nn.Linear(3*self.hd, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.relation_ware = relation_aware\n",
        "        self.leakyrelu = nn.LeakyReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.w_liner = nn.Linear(input_dim, output_dim*head)\n",
        "\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            # Use 'feat' for heterogeneous graph node data\n",
        "            if 'r' in g.ntypes:\n",
        "                g.nodes['r'].data['feat'] = h\n",
        "            else: # Fallback for homogeneous graph if needed\n",
        "                g.ndata['feat'] = h\n",
        "\n",
        "            # Apply edge function on the given etype, specifying etype for heterogeneous graphs\n",
        "            g.apply_edges(self.sign_edges, etype=self.etype)\n",
        "            edge_sum = g.edges[self.etype].data['edge_sum']\n",
        "\n",
        "            h = self.w_liner(h)\n",
        "            # Use 'h' for node data in update_all\n",
        "            if 'r' in g.ntypes:\n",
        "                g.nodes['r'].data['h'] = h\n",
        "            else: # Fallback for homogeneous graph\n",
        "                g.ndata['h'] = h\n",
        "\n",
        "            # Update all, specifying etype for heterogeneous graphs\n",
        "            g.update_all(message_func=self.message, reduce_func=self.reduce, etype=self.etype)\n",
        "\n",
        "            # Retrieve output and edge_sum from node data\n",
        "            if 'r' in g.ntypes:\n",
        "                out = g.nodes['r'].data['out']\n",
        "                edge_s = g.nodes['r'].data['s']\n",
        "            else: # Fallback for homogeneous graph\n",
        "                out = g.ndata['out']\n",
        "                edge_s = g.ndata['s']\n",
        "\n",
        "\n",
        "            if not self.if_sum:\n",
        "                return edge_s, out, h.view(-1, self.head*self.hd)\n",
        "            else:\n",
        "                return edge_s, out, h.view(-1, self.head, self.hd).sum(-2)\n",
        "\n",
        "\n",
        "    def message(self, edges):\n",
        "        src_f = edges.src['h']\n",
        "        src_f = src_f.view(-1, self.head, self.hd)\n",
        "        dst_f = edges.dst['h']\n",
        "        dst_f = dst_f.view(-1, self.head, self.hd)\n",
        "        edge_s = edges.data['edge_sum'].view(-1, self.head, self.hd)\n",
        "        z = torch.cat([src_f, dst_f, edge_s], dim=-1)\n",
        "\n",
        "        alpha = self.atten(z)\n",
        "        alpha = self.leakyrelu(alpha)\n",
        "        return {'atten':alpha, 'sf':src_f, 'edge_s': edge_s}\n",
        "\n",
        "\n",
        "    def reduce(self, nodes):\n",
        "        alpha = nodes.mailbox['atten']\n",
        "        sf = nodes.mailbox['sf']\n",
        "        alpha = self.softmax(alpha)\n",
        "        out = torch.sum(alpha*sf, dim=1)\n",
        "        if not self.if_sum:\n",
        "            out = out.view(-1, self.head*self.hd)\n",
        "            edge_s = torch.mean(nodes.mailbox['edge_s'], dim=1).view(-1, self.head*self.hd)\n",
        "            return {'out':out, 's': edge_s}\n",
        "        else:\n",
        "            out = out.sum(dim=-2)\n",
        "            edge_s = torch.sum(torch.mean(nodes.mailbox['edge_s'], dim=1), dim=-2)\n",
        "\n",
        "            return {'out':out, 's': edge_s}\n",
        "\n",
        "\n",
        "    def sign_edges(self, edges):\n",
        "        # Access features from the edges object directly\n",
        "        src = edges.src['feat']\n",
        "        dst = edges.dst['feat']\n",
        "\n",
        "        edge_sum = self.relation_ware(src, dst)\n",
        "        return {'edge_sum':edge_sum}\n",
        "\n",
        "class Gate(nn.Module):\n",
        "    def __init__(self, head, output_dim, dropout, if_sum=False):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.head = head\n",
        "        if not if_sum:\n",
        "            self.beta = nn.Parameter(torch.empty(size=(2*self.head*self.output_dim, 1)))\n",
        "            nn.init.xavier_normal_(self.beta.data, gain=1.414)\n",
        "        else:\n",
        "\n",
        "            self.beta = nn.Parameter(torch.empty(size=(2*self.output_dim, 1)))\n",
        "            nn.init.xavier_normal_(self.beta.data, gain=1.414)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, edge_sum, out, h):\n",
        "        beta = torch.cat([edge_sum, out], dim=1)\n",
        "        gate = self.sigmoid(torch.matmul(beta, self.beta))\n",
        "        final = gate * out + (1 - gate) * h\n",
        "        return final\n",
        "\n",
        "\n",
        "class RelationAware(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.d_liner = nn.Linear(input_dim, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, dst):\n",
        "        src = self.d_liner(src)\n",
        "        dst = self.d_liner(dst)\n",
        "        diff = src-dst\n",
        "        edge_sum = self.tanh(src + dst + diff)\n",
        "        return edge_sum\n",
        "\n",
        "\n",
        "class MultiRelationGE_GNNLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, head, dataset, dropout, if_sum=False):\n",
        "        super().__init__()\n",
        "        self.relation = copy.deepcopy(dataset.etypes)\n",
        "        if 'homo' in self.relation:\n",
        "            self.relation.remove('homo')\n",
        "        self.n_relation = len(self.relation)\n",
        "        self.if_sum = if_sum\n",
        "        if not self.if_sum:\n",
        "            self.liner = nn.Linear(self.n_relation*output_dim*head, output_dim*head)\n",
        "        else:\n",
        "            self.liner = nn.Linear(self.n_relation*output_dim, output_dim)\n",
        "        self.relation_aware = RelationAware(input_dim, output_dim*head, dropout)\n",
        "\n",
        "        self.minelayers = nn.ModuleDict()\n",
        "        self.sublayer = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "#         if not self.if_sum:\n",
        "        for e in self.relation:\n",
        "            self.sublayer = nn.ModuleList()\n",
        "            self.minelayers[e] = self.sublayer\n",
        "            self.sublayer.append(H_layer(input_dim, output_dim, head, self.relation_aware, e, dropout, if_sum))\n",
        "            self.sublayer.append(Gate(head, output_dim, dropout, if_sum))\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        hs = []\n",
        "        for e in self.relation:\n",
        "            edge_sum1, out1, h1 = self.minelayers[e][0](g, h)\n",
        "            he = self.minelayers[e][1](edge_sum1, out1, h1)\n",
        "            hs.append(he)\n",
        "        x = torch.cat(hs, dim=1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.liner(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, g, h):\n",
        "        with g.local_scope():\n",
        "            # Use 'feat' for heterogeneous graph node data\n",
        "            if 'r' in g.ntypes:\n",
        "                g.nodes['r'].data['feat'] = h\n",
        "            else: # Fallback for homogeneous graph\n",
        "                g.ndata['feat'] = h\n",
        "\n",
        "            # Use correct mask names for heterogeneous graph\n",
        "            if 'r' in g.ntypes:\n",
        "                train_mask = g.nodes['r'].data['trn_msk'].bool()\n",
        "                labels = g.nodes['r'].data['label']\n",
        "            else: # Fallback for homogeneous graph\n",
        "                train_mask = g.ndata['train_mask'].bool()\n",
        "                labels = g.ndata['label']\n",
        "\n",
        "\n",
        "            train_label = labels[train_mask]\n",
        "            train_pos = train_label==1\n",
        "            train_neg = train_label==0\n",
        "            train_pos_index = train_pos.nonzero().flatten().detach().cpu().numpy()\n",
        "            train_neg_index = train_neg.nonzero().flatten().detach().cpu().numpy()\n",
        "            # Handle case where there might be fewer negative samples than positive\n",
        "            size_neg_sample = min(len(train_pos_index), len(train_neg_index))\n",
        "            train_neg_index = np.random.choice(train_neg_index, size=size_neg_sample, replace=False)\n",
        "\n",
        "            node_index = np.concatenate([train_neg_index, train_pos_index])\n",
        "            np.random.shuffle(node_index) # Shuffle the indices\n",
        "\n",
        "            hs = []\n",
        "            diff_loss = 0\n",
        "            for e in self.relation:\n",
        "                edge_sum1, out1, h1 = self.minelayers[e][0](g, h)\n",
        "                he = self.minelayers[e][1](edge_sum1, out1, h1)\n",
        "                hs.append(he)\n",
        "\n",
        "#                 diff_loss += diff_loss1\n",
        "            x = torch.cat(hs, dim=1)\n",
        "            x = self.dropout(x)\n",
        "            agg_h = self.liner(x)\n",
        "#             diff_loss1 = F.cross_entropy(agg_h[train_mask][node_index], train_label[node_index])\n",
        "            if not self.if_sum:\n",
        "                # Ensure indices are applied correctly after masking\n",
        "                masked_agg_h = agg_h[train_mask]\n",
        "                valid_indices = node_index[node_index < masked_agg_h.shape[0]]\n",
        "                diff_loss1 = F.cross_entropy(masked_agg_h[valid_indices], train_label[valid_indices])\n",
        "            else:\n",
        "                # Ensure indices are applied correctly after masking\n",
        "                masked_agg_h = agg_h[train_mask]\n",
        "                valid_indices = node_index[node_index < masked_agg_h.shape[0]]\n",
        "                diff_loss1 = F.cross_entropy(masked_agg_h[valid_indices], train_label[valid_indices])\n",
        "#             diff_loss2 = F.cross_entropy(self.filter2(h[train_mask][node_index]), train_label[node_index])\n",
        "#             diff_loss3 = F.cross_entropy(h[train_mask][node_index], train_label[node_index])\n",
        "            return agg_h, diff_loss1\n",
        "\n",
        "\n",
        "\n",
        "class GE_GNN(nn.Module):\n",
        "    def __init__(self, args, g):\n",
        "        super().__init__()\n",
        "        self.n_layer = args.n_layer\n",
        "        # Determine input_dim based on graph features, prioritizing heterogeneous node data\n",
        "        if hasattr(g, 'ntypes') and 'r' in g.ntypes and 'feat' in g.nodes['r'].data: # Check for 'feat' in heterogeneous graph\n",
        "            self.input_dim = g.nodes['r'].data['feat'].shape[1]\n",
        "        elif 'feature' in g.ndata: # Fallback to homogeneous graph 'feature'\n",
        "            self.input_dim = g.ndata['feature'].shape[1]\n",
        "        else:\n",
        "            # Fallback or raise error if features are not found\n",
        "            print(\"Warning: 'feat' not found in nodes['r'].data and 'feature' not found in ndata. Assuming input_dim from config if available.\")\n",
        "            self.input_dim = getattr(args, 'input_dim', 0) # Use config if available, default to 0\n",
        "            if self.input_dim == 0:\n",
        "                 raise RuntimeError(\"Graph does not contain 'feat' node data (heterogeneous) or 'feature' node data (homogeneous) and 'input_dim' not specified in config.\")\n",
        "\n",
        "\n",
        "        self.intra_dim = args.intra_dim\n",
        "        self.n_class = args.n_class\n",
        "        self.gamma1 = args.gamma1\n",
        "        self.gamma2 = args.gamma2\n",
        "        self.n_layer = args.n_layer\n",
        "        self.mine_layers = nn.ModuleList()\n",
        "        if args.n_layer == 1:\n",
        "            self.mine_layers.append(MultiRelationGE_GNNLayer(self.input_dim, self.n_class, args.head, g, args.dropout, if_sum=True))\n",
        "        else:\n",
        "            self.mine_layers.append(MultiRelationGE_GNNLayer(self.input_dim, self.intra_dim, args.head, g, args.dropout))\n",
        "            for _ in range(1, self.n_layer-1):\n",
        "                self.mine_layers.append(MultiRelationGE_GNNLayer(self.intra_dim*args.head, self.intra_dim, args.head, g, args.dropout))\n",
        "            self.mine_layers.append(MultiRelationGE_GNNLayer(self.intra_dim*args.head, self.n_class, args.head, g, args.dropout, if_sum=True))\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, g):\n",
        "        # Get features from the graph, handling heterogeneous graphs\n",
        "        if 'r' in g.ntypes and 'feat' in g.nodes['r'].data: # Prioritize heterogeneous graph 'feat'\n",
        "             feats = g.nodes['r'].data['feat'].float()\n",
        "        elif 'feature' in g.ndata: # Fallback to homogeneous graph 'feature'\n",
        "            feats = g.ndata['feature'].float()\n",
        "        else:\n",
        "            raise RuntimeError(\"Graph missing 'feat' in nodes['r'].data or 'feature' in ndata for forward pass.\")\n",
        "\n",
        "        h = feats # Start with initial features as input to the first layer\n",
        "        for i, layer in enumerate(self.mine_layers):\n",
        "            h = layer(g, h)\n",
        "            if i < len(self.mine_layers) - 1: # Apply ReLU and Dropout for intermediate layers\n",
        "                h = self.relu(h)\n",
        "                h = self.dropout(h)\n",
        "        return h\n",
        "\n",
        "    def loss(self, g):\n",
        "        # Compute training loss.\n",
        "        # This loss function structure was copied from the original code's MultiRelationGE_GNNLayer.\n",
        "        # To fully replicate the original training, this logic should likely be adapted or moved\n",
        "        # to a training loop that utilizes the forward pass and calculates loss based on model outputs and labels.\n",
        "        # For now, I will provide a basic loss calculation based on the model's forward pass\n",
        "        # and available training masks/labels, assuming the required data is in g.ndata or g.nodes['r'].data.\n",
        "\n",
        "        # Get features from the graph, handling heterogeneous graphs\n",
        "        if 'r' in g.ntypes and 'feat' in g.nodes['r'].data: # Prioritize heterogeneous graph 'feat'\n",
        "             feats = g.nodes['r'].data['feat'].float()\n",
        "        elif 'feature' in g.ndata: # Fallback to homogeneous graph 'feature'\n",
        "            feats = g.ndata['feature'].float()\n",
        "        else:\n",
        "            raise RuntimeError(\"Graph missing 'feat' in nodes['r'].data or 'feature' in ndata for loss calculation.\")\n",
        "\n",
        "\n",
        "        # Use correct mask names for heterogeneous graph\n",
        "        if 'r' in g.ntypes and 'trn_msk' in g.nodes['r'].data:\n",
        "             train_mask = g.nodes['r'].data['trn_msk'].bool()\n",
        "             labels = g.nodes['r'].data['label']\n",
        "        elif 'train_mask' in g.ndata:\n",
        "            train_mask = g.ndata['train_mask'].bool()\n",
        "            labels = g.ndata['label']\n",
        "        else:\n",
        "             print(\"Error: Graph missing 'trn_msk' node data for loss calculation.\")\n",
        "             return torch.tensor(0.0, device=g.device)\n",
        "\n",
        "\n",
        "        train_label = labels[train_mask]\n",
        "        train_pos = train_label == 1\n",
        "        train_neg = train_label == 0\n",
        "\n",
        "        pos_index = train_pos.nonzero().flatten().detach().cpu().numpy()\n",
        "        neg_index = train_neg.nonzero().flatten().detach().cpu().numpy()\n",
        "        # Handle case where there might be fewer negative samples than positive\n",
        "        size_neg_sample = min(len(pos_index), len(neg_index))\n",
        "        neg_index = np.random.choice(neg_index, size=size_neg_sample, replace=False)\n",
        "\n",
        "        index = np.concatenate([pos_index, neg_index])\n",
        "        np.random.shuffle(index) # Shuffle the indices\n",
        "\n",
        "        h = feats # Start with initial features as input to the first layer\n",
        "        prototype_loss = 0\n",
        "        for i, layer in enumerate(self.mine_layers):\n",
        "            # Need to pass the graph 'g' and the current hidden state 'h' to the layer's loss method\n",
        "            # The layer's loss method also returns the updated hidden state 'h'\n",
        "            if hasattr(layer, 'loss'): # Check if the layer has a loss method (MultiRelationGE_GNNLayer does)\n",
        "                 h, p_loss = layer.loss(g, h) # Pass graph and current hidden state\n",
        "                 prototype_loss += p_loss\n",
        "            else:\n",
        "                # If the layer doesn't have a specific loss method, just perform forward pass\n",
        "                h = layer(g, h)\n",
        "\n",
        "            if i < len(self.mine_layers) - 1: # Apply ReLU and Dropout for intermediate layers\n",
        "                h = self.relu(h)\n",
        "                h = self.dropout(h)\n",
        "\n",
        "\n",
        "        # Ensure indices are applied correctly after masking\n",
        "        masked_h = h[train_mask]\n",
        "        valid_indices = index[index < masked_h.shape[0]] # Filter indices to be within masked tensor bounds\n",
        "        model_loss = F.cross_entropy(masked_h[valid_indices], train_label[valid_indices])\n",
        "\n",
        "        # Note: The original code had '1.2 * prototype_loss / 3'. Adjust multiplier as needed based on the intended weighting.\n",
        "        # Assuming the sum of prototype_loss from each layer is desired.\n",
        "        loss = model_loss + 1.2 * prototype_loss # Adjust multiplier as needed\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Load args from YAML\n",
        "config_path = \"/content/drive/MyDrive/GE-GNN/config/amazon.yaml\" # Use the specified config path\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "args = argparse.Namespace(**config) # Create args namespace from config\n",
        "\n",
        "setup_seed(args.seed)\n",
        "device = torch.device(args.cuda if torch.cuda.is_available() and args.cuda else 'cpu')\n",
        "args.device = device\n",
        "\n",
        "# --- Load the specified DGL graph ---\n",
        "dgl_graph_path = \"/content/drive/MyDrive/GE-GNN/Mat_Dgl_files/sports_outdoors_hetero_pavan_20k_F.dgl\"\n",
        "try:\n",
        "    graphs, labels_dict = dgl.load_graphs(dgl_graph_path)\n",
        "    g = graphs[0] # Assuming the DGL file contains a list of graphs, and we want the first one\n",
        "    print(f\"Loaded DGL graph from: {dgl_graph_path}\")\n",
        "\n",
        "    # Ensure graph and node data are on the correct device\n",
        "    g = g.to(args.device)\n",
        "    if 'r' in g.ntypes:\n",
        "        for key in g.nodes['r'].data:\n",
        "            g.nodes['r'].data[key] = g.nodes['r'].data[key].to(args.device)\n",
        "    # If it's a homogeneous graph (fallback, though we expect heterogeneous)\n",
        "    else:\n",
        "         for key in g.ndata:\n",
        "            g.ndata[key] = g.ndata[key].to(args.device)\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: DGL graph file not found at {dgl_graph_path}\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the DGL graph: {e}\")\n",
        "    sys.exit()\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# Set dataset_name for output file naming\n",
        "dataset_name = \"sports_outdoors_20k_F\" # Explicitly set dataset name for consistent output file naming\n",
        "\n",
        "\n",
        "# Create results directory if it doesn't exist\n",
        "if not os.path.exists(args.result_path):\n",
        "    os.makedirs(args.result_path)\n",
        "\n",
        "'''\n",
        "# train model\n",
        "'''\n",
        "print('Start training model...')\n",
        "model = GE_GNN(args, g) # Pass the loaded graph 'g' to the model constructor\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "early_stop = EarlyStop(args.early_stop)\n",
        "\n",
        "# gradient clip\n",
        "max_grad_norm = 5.0\n",
        "\n",
        "\n",
        "for e in range(args.epoch):\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    # Calculate loss using the model's loss method, passing the graph 'g'\n",
        "    loss = model.loss(g)\n",
        "    loss.backward()\n",
        "    # gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        '''\n",
        "        # valid\n",
        "        '''\n",
        "        model.eval()\n",
        "        # Use correct mask names for heterogeneous graph\n",
        "        if 'r' in g.ntypes and 'val_msk' in g.nodes['r'].data:\n",
        "             valid_mask = g.nodes['r'].data['val_msk'].bool()\n",
        "             valid_labels = g.nodes['r'].data['label'][valid_mask].cpu().numpy()\n",
        "             # Corrected: Perform forward pass to get logits for validation\n",
        "             logits = model(g)[valid_mask]\n",
        "        elif 'valid_mask' in g.ndata: # Fallback for homogeneous graph\n",
        "            valid_mask = g.ndata['valid_mask'].bool()\n",
        "            valid_labels = g.ndata['label'][valid_mask].cpu().numpy()\n",
        "            # Corrected: Perform forward pass to get logits for validation\n",
        "            logits = model(g)[valid_mask]\n",
        "        else:\n",
        "             print(\"Warning: 'val_msk' not found in graph. Skipping validation.\")\n",
        "             continue # Skip validation if mask is missing\n",
        "\n",
        "\n",
        "        if len(valid_labels) == 0:\n",
        "             print(\"Warning: No validation samples found. Skipping validation metrics.\")\n",
        "             auc = 0.0 # Assign a default low AUC\n",
        "             f1_macro, gmean, recall = 0.0, 0.0, 0.0\n",
        "        else:\n",
        "            # Corrected: Pass the calculated logits to the evaluate function\n",
        "            f1_macro, auc, gmean, recall = evaluate(valid_labels, logits)\n",
        "\n",
        "        if args.log:\n",
        "            print(f'{e}: Best Epoch:{early_stop.best_epoch}, Best valid AUC:{early_stop.best_eval}, Loss:{loss.item()}, Current valid: Recall:{recall:.4f}, F1_macro:{f1_macro:.4f}, G-Mean:{gmean:.4f}, AUC:{auc:.6f}')\n",
        "\n",
        "        do_store, do_stop = early_stop.step(auc, e)\n",
        "        if do_store:\n",
        "            # Save model state dict\n",
        "            model_save_path = os.path.join(args.result_path, f\"{dataset_name}_model_head{args.head}.pt\")\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            if args.log:\n",
        "                 print(f\"Saved best model state_dict for {dataset_name} at epoch {e}, best_auc={early_stop.best_eval:.6f}\")\n",
        "\n",
        "        if do_stop:\n",
        "            break\n",
        "print('End training')\n",
        "\n",
        "'''\n",
        "# test model\n",
        "'''\n",
        "print('Test model...')\n",
        "# Load model state dict\n",
        "model_load_path = os.path.join(args.result_path, f\"{dataset_name}_model_head{args.head}.pt\")\n",
        "if os.path.exists(model_load_path):\n",
        "    # Create a new model instance before loading state dict\n",
        "    model = GE_GNN(args, g) # Pass the graph again to initialize correctly\n",
        "    model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Use correct mask names for heterogeneous graph\n",
        "        if 'r' in g.ntypes and 'tst_msk' in g.nodes['r'].data:\n",
        "            test_mask = g.nodes['r'].data['tst_msk'].bool()\n",
        "            test_labels = g.nodes['r'].data['label'][test_mask]\n",
        "            test_labels = test_labels.cpu().numpy()\n",
        "            logits = model(g)[test_mask]\n",
        "            logits = logits.cpu()\n",
        "        elif 'test_mask' in g.ndata: # Fallback for homogeneous graph\n",
        "            test_mask = g.ndata['test_mask'].bool()\n",
        "            test_labels = g.ndata['label'][test_mask]\n",
        "            test_labels = test_labels.cpu().numpy()\n",
        "            logits = model(g)[test_mask]\n",
        "            logits = logits.cpu()\n",
        "        else:\n",
        "            print(f\"Warning: 'tst_msk' not found in graph for {dataset_name}. Skipping test evaluation.\")\n",
        "            sys.exit() # Exit if test mask is missing\n",
        "\n",
        "\n",
        "        test_result_path = os.path.join(args.result_path, dataset_name)\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"Warning: No test samples found for {dataset_name}. Skipping test metrics.\")\n",
        "        else:\n",
        "            f1_macro, auc, gmean, recall = evaluate(test_labels, logits, test_result_path)\n",
        "            print(f'Test: F1-macro:{f1_macro:.6f}, AUC:{auc:.6f}, G-Mean:{gmean:.6f}, Recall:{recall:.6f}')\n",
        "else:\n",
        "    print(f\"No saved model found at {model_load_path}; skipping test for {dataset_name}.\")\n",
        "\n",
        "# exit()"
      ],
      "metadata": {
        "id": "Zkaypr7cpxI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# GE-GNN Training (Leak-Safe)\n",
        "# ===========================\n",
        "\n",
        "import os, sys, copy, random, warnings, argparse, yaml\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import dgl\n",
        "\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, recall_score\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Reproducibility helper\n",
        "# ----------------------\n",
        "def setup_seed(seed: int):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# ----------------\n",
        "# EarlyStop helper\n",
        "# ----------------\n",
        "class EarlyStop:\n",
        "    def __init__(self, early_stop: int, if_more: bool = True):\n",
        "        self.best_eval = -float('inf') if if_more else float('inf')\n",
        "        self.best_epoch = 0\n",
        "        self.if_more = if_more\n",
        "        self.early_stop = early_stop\n",
        "        self.stop_steps = 0\n",
        "\n",
        "    def step(self, current_eval: float, current_epoch: int):\n",
        "        do_stop, do_store = False, False\n",
        "        better = current_eval > self.best_eval if self.if_more else current_eval < self.best_eval\n",
        "        if better:\n",
        "            self.best_eval = current_eval\n",
        "            self.best_epoch = current_epoch\n",
        "            self.stop_steps = 0\n",
        "            do_store = True\n",
        "        else:\n",
        "            self.stop_steps += 1\n",
        "            if self.stop_steps >= self.early_stop:\n",
        "                do_stop = True\n",
        "        return do_store, do_stop\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Metrics / Eval helper\n",
        "# ---------------------\n",
        "def _conf_gmean(conf):\n",
        "    tn, fp, fn, tp = conf.ravel()\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    return (tpr * tnr) ** 0.5\n",
        "\n",
        "\n",
        "def evaluate(labels_np, logits, result_prefix: str = ''):\n",
        "    probs = F.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n",
        "    preds = logits.argmax(1).detach().cpu().numpy()\n",
        "\n",
        "    if result_prefix:\n",
        "        np.save(result_prefix + '_result_preds.npy', preds)\n",
        "        np.save(result_prefix + '_result_probs.npy', probs)\n",
        "\n",
        "    if len(np.unique(labels_np)) < 2:\n",
        "        auc = 0.5\n",
        "    else:\n",
        "        auc = roc_auc_score(labels_np, probs)\n",
        "\n",
        "    conf = confusion_matrix(labels_np, preds, labels=[0, 1])\n",
        "    recall = recall_score(labels_np, preds, zero_division=0)\n",
        "    f1_macro = f1_score(labels_np, preds, average='macro', zero_division=0)\n",
        "    gmean = _conf_gmean(conf)\n",
        "    return f1_macro, auc, gmean, recall\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Graph access utilities\n",
        "# ----------------------\n",
        "def has_r(g):\n",
        "    return hasattr(g, 'ntypes') and ('r' in g.ntypes)\n",
        "\n",
        "\n",
        "def get_feat_key(g):\n",
        "    \"\"\"\n",
        "    Return the node feature key to read from for initial input.\n",
        "    Preference: 'feature' then 'feat'.\n",
        "    Works for hetero('r') and homo graphs.\n",
        "    \"\"\"\n",
        "    if has_r(g):\n",
        "        keys = g.nodes['r'].data.keys()\n",
        "        if 'feature' in keys:\n",
        "            return ('r', 'feature')\n",
        "        if 'feat' in keys:\n",
        "            return ('r', 'feat')\n",
        "    else:\n",
        "        keys = g.ndata.keys()\n",
        "        if 'feature' in keys:\n",
        "            return (None, 'feature')\n",
        "        if 'feat' in keys:\n",
        "            return (None, 'feat')\n",
        "    raise KeyError(\"No node features found. Expected 'feature' or 'feat'.\")\n",
        "\n",
        "\n",
        "def get_label_and_masks(g):\n",
        "    \"\"\"\n",
        "    Return labels tensor and (train, valid, test) masks as boolean tensors.\n",
        "    Supports both {trn,val,tst}_msk and {train,valid,test}_mask.\n",
        "    Works for hetero('r') and homo graphs.\n",
        "    \"\"\"\n",
        "    if has_r(g):\n",
        "        ndata = g.nodes['r'].data\n",
        "        label_key = 'label'\n",
        "        if label_key not in ndata:\n",
        "            raise KeyError(\"labels not found at nodes['r'].data['label']\")\n",
        "\n",
        "        def pick_mask(prefix_a, prefix_b):\n",
        "            if prefix_a in ndata:\n",
        "                return ndata[prefix_a].bool()\n",
        "            if prefix_b in ndata:\n",
        "                return ndata[prefix_b].bool()\n",
        "            return None\n",
        "\n",
        "        train_mask = pick_mask('trn_msk', 'train_mask')\n",
        "        valid_mask = pick_mask('val_msk', 'valid_mask')\n",
        "        test_mask = pick_mask('tst_msk', 'test_mask')\n",
        "\n",
        "        if train_mask is None or valid_mask is None or test_mask is None:\n",
        "            raise KeyError(\"Could not find train/valid/test masks in nodes['r'].data.\")\n",
        "\n",
        "        labels = ndata[label_key].long()\n",
        "        return labels, train_mask, valid_mask, test_mask\n",
        "    else:\n",
        "        ndata = g.ndata\n",
        "        label_key = 'label'\n",
        "        if label_key not in ndata:\n",
        "            raise KeyError(\"labels not found at g.ndata['label']\")\n",
        "\n",
        "        def pick_mask(prefix_a, prefix_b):\n",
        "            if prefix_a in ndata:\n",
        "                return ndata[prefix_a].bool()\n",
        "            if prefix_b in ndata:\n",
        "                return ndata[prefix_b].bool()\n",
        "            return None\n",
        "\n",
        "        train_mask = pick_mask('trn_msk', 'train_mask')\n",
        "        valid_mask = pick_mask('val_msk', 'valid_mask')\n",
        "        test_mask = pick_mask('tst_msk', 'test_mask')\n",
        "\n",
        "        if train_mask is None or valid_mask is None or test_mask is None:\n",
        "            raise KeyError(\"Could not find train/valid/test masks in g.ndata.\")\n",
        "\n",
        "        labels = ndata[label_key].long()\n",
        "        return labels, train_mask, valid_mask, test_mask\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# GE-GNN Layer primitives\n",
        "# -------------------------\n",
        "class RelationAware(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.d_lin = nn.Linear(input_dim, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, dst):\n",
        "        src = self.d_lin(src)\n",
        "        dst = self.d_lin(dst)\n",
        "        diff = src - dst\n",
        "        return self.tanh(src + dst + diff)\n",
        "\n",
        "\n",
        "class Gate(nn.Module):\n",
        "    def __init__(self, head, output_dim, dropout, if_sum=False):\n",
        "        super().__init__()\n",
        "        self.head = head\n",
        "        self.output_dim = output_dim\n",
        "        if not if_sum:\n",
        "            self.beta = nn.Parameter(torch.empty(size=(2 * self.head * self.output_dim, 1)))\n",
        "        else:\n",
        "            self.beta = nn.Parameter(torch.empty(size=(2 * self.output_dim, 1)))\n",
        "        nn.init.xavier_normal_(self.beta.data, gain=1.414)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, edge_sum, out, h):\n",
        "        beta = torch.cat([edge_sum, out], dim=1)\n",
        "        gate = self.sigmoid(beta @ self.beta)\n",
        "        return gate * out + (1 - gate) * h\n",
        "\n",
        "\n",
        "class HLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    One relation head.\n",
        "    Works for both hetero and homo graphs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, head, relation_aware, etype, dropout, if_sum=False):\n",
        "        super().__init__()\n",
        "        self.etype = etype        # None for homogeneous\n",
        "        self.head = head\n",
        "        self.hd = output_dim\n",
        "        self.if_sum = if_sum\n",
        "        self.relation_aware = relation_aware\n",
        "\n",
        "        self.atten = nn.Linear(3 * self.hd, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leakyrelu = nn.LeakyReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.w_linear = nn.Linear(input_dim, output_dim * head)\n",
        "\n",
        "    def _apply_edges(self, g):\n",
        "        # message fn needs node field 'h_in' present\n",
        "        def sign_edges(edges):\n",
        "            src = edges.src['h_in']\n",
        "            dst = edges.dst['h_in']\n",
        "            edge_sum = self.relation_aware(src, dst)  # shape [E, output_dim*head]\n",
        "            return {'edge_sum': edge_sum}\n",
        "        return sign_edges\n",
        "\n",
        "    def message(self, edges):\n",
        "        src_f = edges.src['h']\n",
        "        dst_f = edges.dst['h']\n",
        "        src_f = src_f.view(-1, self.head, self.hd)\n",
        "        dst_f = dst_f.view(-1, self.head, self.hd)\n",
        "        edge_s = edges.data['edge_sum'].view(-1, self.head, self.hd)\n",
        "        z = torch.cat([src_f, dst_f, edge_s], dim=-1)\n",
        "        alpha = self.leakyrelu(self.atten(z))\n",
        "        return {'atten': alpha, 'sf': src_f, 'edge_s': edge_s}\n",
        "\n",
        "    def reduce(self, nodes):\n",
        "        alpha = self.softmax(nodes.mailbox['atten'])\n",
        "        sf = nodes.mailbox['sf']\n",
        "        out = torch.sum(alpha * sf, dim=1)\n",
        "        if not self.if_sum:\n",
        "            out = out.view(-1, self.head * self.hd)\n",
        "            edge_s = torch.mean(nodes.mailbox['edge_s'], dim=1).view(-1, self.head * self.hd)\n",
        "            return {'out': out, 's': edge_s}\n",
        "        else:\n",
        "            out = out.sum(dim=-2)\n",
        "            edge_s = torch.sum(torch.mean(nodes.mailbox['edge_s'], dim=1), dim=-2)\n",
        "            return {'out': out, 's': edge_s}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            # project features for messages\n",
        "            h_proj = self.w_linear(h)\n",
        "\n",
        "            # set working fields\n",
        "            if has_r(g):\n",
        "                g.nodes['r'].data['h_in'] = h          # for edge signing\n",
        "                g.nodes['r'].data['h'] = h_proj        # for message passing\n",
        "                if self.etype is None:\n",
        "                    # shouldn't happen for hetero; but keep guard\n",
        "                    g.apply_edges(self._apply_edges(g))\n",
        "                    g.update_all(self.message, self.reduce)\n",
        "                    out = g.nodes['r'].data['out']\n",
        "                    edge_s = g.nodes['r'].data['s']\n",
        "                else:\n",
        "                    g.apply_edges(self._apply_edges(g), etype=self.etype)\n",
        "                    g.update_all(self.message, self.reduce, etype=self.etype)\n",
        "                    out = g.nodes['r'].data['out']\n",
        "                    edge_s = g.nodes['r'].data['s']\n",
        "            else:\n",
        "                g.ndata['h_in'] = h\n",
        "                g.ndata['h'] = h_proj\n",
        "                g.apply_edges(self._apply_edges(g))\n",
        "                g.update_all(self.message, self.reduce)\n",
        "                out = g.ndata['out']\n",
        "                edge_s = g.ndata['s']\n",
        "\n",
        "            if not self.if_sum:\n",
        "                return edge_s, out, h_proj.view(-1, self.head * self.hd)\n",
        "            else:\n",
        "                return edge_s, out, h_proj.view(-1, self.head, self.hd).sum(-2)\n",
        "\n",
        "\n",
        "class MultiRelationGE_GNNLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, head, graph_for_etypes, dropout, if_sum=False):\n",
        "        super().__init__()\n",
        "        # collect relations\n",
        "        etypes = list(graph_for_etypes.etypes) if hasattr(graph_for_etypes, 'etypes') else []\n",
        "        # For homogeneous graphs, etypes could be ['_E'] or empty; treat as single relation None\n",
        "        self.relations = etypes if (has_r(graph_for_etypes) and len(etypes) > 0) else [None]\n",
        "        self.if_sum = if_sum\n",
        "        self.n_rel = len(self.relations)\n",
        "\n",
        "        self.lin = nn.Linear(self.n_rel * (output_dim if if_sum else output_dim * head),\n",
        "                             (output_dim if if_sum else output_dim * head))\n",
        "\n",
        "        self.rel_aware = RelationAware(input_dim, output_dim * head, dropout)\n",
        "        self.blocks = nn.ModuleDict()\n",
        "        for e in self.relations:\n",
        "            block = nn.ModuleList([\n",
        "                HLayer(input_dim, output_dim, head, self.rel_aware, e, dropout, if_sum),\n",
        "                Gate(head, output_dim, dropout, if_sum)\n",
        "            ])\n",
        "            self.blocks[str(e)] = block\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        hs = []\n",
        "        for e in self.relations:\n",
        "            edge_sum, out, h_lin = self.blocks[str(e)][0](g, h)\n",
        "            he = self.blocks[str(e)][1](edge_sum, out, h_lin)\n",
        "            hs.append(he)\n",
        "        x = torch.cat(hs, dim=1) if len(hs) > 1 else hs[0]\n",
        "        x = self.drop(x)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "# -------------\n",
        "# GE-GNN model\n",
        "# -------------\n",
        "class GE_GNN(nn.Module):\n",
        "    def __init__(self, args, g, n_class=2):\n",
        "        super().__init__()\n",
        "        # input dim\n",
        "        ntype, feat_key = get_feat_key(g)\n",
        "        if has_r(g):\n",
        "            in_dim = g.nodes['r'].data[feat_key].shape[1] if ntype == 'r' else g.ndata[feat_key].shape[1]\n",
        "        else:\n",
        "            in_dim = g.ndata[feat_key].shape[1]\n",
        "\n",
        "        self.n_layer = args.n_layer\n",
        "        self.intra_dim = args.intra_dim\n",
        "        self.head = args.head\n",
        "        self.n_class = n_class\n",
        "        self.drop = nn.Dropout(args.dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        layers = []\n",
        "        if self.n_layer == 1:\n",
        "            layers.append(MultiRelationGE_GNNLayer(in_dim, self.n_class, self.head, g, args.dropout, if_sum=True))\n",
        "        else:\n",
        "            layers.append(MultiRelationGE_GNNLayer(in_dim, self.intra_dim, self.head, g, args.dropout, if_sum=False))\n",
        "            for _ in range(1, self.n_layer - 1):\n",
        "                layers.append(MultiRelationGE_GNNLayer(self.intra_dim * self.head, self.intra_dim, self.head, g, args.dropout, if_sum=False))\n",
        "            layers.append(MultiRelationGE_GNNLayer(self.intra_dim * self.head, self.n_class, self.head, g, args.dropout, if_sum=True))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def _read_init_feat(self, g):\n",
        "        ntype, feat_key = get_feat_key(g)\n",
        "        if has_r(g) and ntype == 'r':\n",
        "            return g.nodes['r'].data[feat_key].float()\n",
        "        elif not has_r(g):\n",
        "            return g.ndata[feat_key].float()\n",
        "        else:\n",
        "            raise RuntimeError(\"Cannot find initial features for forward().\")\n",
        "\n",
        "    def forward(self, g):\n",
        "        h = self._read_init_feat(g)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(g, h)\n",
        "            if i < len(self.layers) - 1:\n",
        "                h = self.relu(h)\n",
        "                h = self.drop(h)\n",
        "        return h  # logits\n",
        "\n",
        "\n",
        "# ---------------\n",
        "# Config / Paths\n",
        "# ---------------\n",
        "# If you prefer YAML: set config_path and load. Otherwise hardcode here.\n",
        "config_path = \"/content/drive/MyDrive/GE-GNN/config/amazon.yaml\"\n",
        "with open(config_path, 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# You can override unstable params here if needed:\n",
        "cfg.setdefault('lr', 0.003)               # a bit lower than 0.01 for stability\n",
        "cfg.setdefault('weight_decay', 5e-4)\n",
        "cfg.setdefault('epoch', 300)\n",
        "cfg.setdefault('early_stop', 50)\n",
        "cfg.setdefault('dropout', 0.3)\n",
        "cfg.setdefault('head', 4)\n",
        "cfg.setdefault('n_layer', 3)\n",
        "cfg.setdefault('intra_dim', 64)\n",
        "cfg.setdefault('n_class', 2)\n",
        "cfg.setdefault('seed', 42)\n",
        "cfg.setdefault('cuda', 0)\n",
        "cfg.setdefault('result_path', \"/content/drive/MyDrive/GE-GNN/result\")\n",
        "\n",
        "args = argparse.Namespace(**cfg)\n",
        "\n",
        "setup_seed(args.seed)\n",
        "device = torch.device(f'cuda:{args.cuda}' if torch.cuda.is_available() else 'cpu')\n",
        "args.device = device\n",
        "\n",
        "dgl_graph_path = \"/content/drive/MyDrive/GE-GNN/Mat_Dgl_files/sports_outdoors_hetero_pavan_20k_F.dgl\"\n",
        "dataset_name = \"sports_outdoors_20k_F\"\n",
        "\n",
        "# -------------\n",
        "# Load the graph\n",
        "# -------------\n",
        "try:\n",
        "    graphs, _ = dgl.load_graphs(dgl_graph_path)\n",
        "    g = graphs[0]\n",
        "    print(f\"Loaded DGL graph from: {dgl_graph_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load graph: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Move to device & ensure node data on device\n",
        "g = g.to(device)\n",
        "if has_r(g):\n",
        "    for k in list(g.nodes['r'].data.keys()):\n",
        "        g.nodes['r'].data[k] = g.nodes['r'].data[k].to(device)\n",
        "else:\n",
        "    for k in list(g.ndata.keys()):\n",
        "        g.ndata[k] = g.ndata[k].to(device)\n",
        "\n",
        "# --------\n",
        "# Training\n",
        "# --------\n",
        "os.makedirs(args.result_path, exist_ok=True)\n",
        "\n",
        "labels, train_mask, valid_mask, test_mask = get_label_and_masks(g)\n",
        "labels_np_valid = labels[valid_mask].detach().cpu().numpy()\n",
        "\n",
        "print('Start training model...')\n",
        "model = GE_GNN(args, g, n_class=args.n_class).to(device)\n",
        "\n",
        "# Class weights (optional but helpful if class-imbalance)\n",
        "with torch.no_grad():\n",
        "    y_train = labels[train_mask].detach().cpu().numpy()\n",
        "    # avoid div by zero; if balanced, weights ~1\n",
        "    pos = (y_train == 1).sum()\n",
        "    neg = (y_train == 0).sum()\n",
        "    if pos > 0 and neg > 0:\n",
        "        w0 = 0.5 * (pos + neg) / neg\n",
        "        w1 = 0.5 * (pos + neg) / pos\n",
        "        class_weight = torch.tensor([w0, w1], dtype=torch.float32, device=device)\n",
        "    else:\n",
        "        class_weight = None\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "early_stop = EarlyStop(args.early_stop, if_more=True)\n",
        "max_grad_norm = 5.0\n",
        "\n",
        "best_model_path = os.path.join(args.result_path, f\"{dataset_name}_model_head{args.head}.pt\")\n",
        "\n",
        "for e in range(args.epoch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = model(g)\n",
        "    if train_mask.sum() == 0:\n",
        "        print(\"No training nodes in mask; aborting.\")\n",
        "        break\n",
        "    loss = F.cross_entropy(logits[train_mask], labels[train_mask], weight=class_weight)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "\n",
        "    # VALID\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if valid_mask.sum() > 0:\n",
        "            val_logits = logits[valid_mask]  # reuse forward result for speed\n",
        "            f1_macro, auc, gmean, recall = evaluate(labels_np_valid, val_logits)\n",
        "        else:\n",
        "            f1_macro, auc, gmean, recall = (0.0, 0.0, 0.0, 0.0)\n",
        "\n",
        "    print(f\"{e}: Best Epoch:{early_stop.best_epoch}, Best valid AUC:{early_stop.best_eval:.6f}, \"\n",
        "          f\"Loss:{loss.item():.6f}, Current valid: Recall:{recall:.4f}, \"\n",
        "          f\"F1_macro:{f1_macro:.4f}, G-Mean:{gmean:.4f}, AUC:{auc:.6f}\")\n",
        "\n",
        "    do_store, do_stop = early_stop.step(auc, e)\n",
        "    if do_store:\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Saved best model state_dict for {dataset_name} at epoch {e}, best_auc={early_stop.best_eval:.6f}\")\n",
        "    if do_stop:\n",
        "        print(f\"Early stopping at epoch {e}. Best epoch: {early_stop.best_epoch}\")\n",
        "        break\n",
        "\n",
        "print('End training')\n",
        "\n",
        "# ----\n",
        "# Test\n",
        "# ----\n",
        "print('Test model...')\n",
        "if os.path.exists(best_model_path):\n",
        "    model = GE_GNN(args, g, n_class=args.n_class).to(device)\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(g)\n",
        "        if test_mask.sum() == 0:\n",
        "            print(\"No test nodes found; skipping test.\")\n",
        "        else:\n",
        "            test_logits = logits[test_mask]\n",
        "            test_labels_np = labels[test_mask].detach().cpu().numpy()\n",
        "            test_prefix = os.path.join(args.result_path, dataset_name)\n",
        "            f1_macro, auc, gmean, recall = evaluate(test_labels_np, test_logits, test_prefix)\n",
        "            print(f\"Test: F1-macro:{f1_macro:.6f}, AUC:{auc:.6f}, G-Mean:{gmean:.6f}, Recall:{recall:.6f}\")\n",
        "else:\n",
        "    print(f\"No saved model found at {best_model_path}; skipping test.\")\n"
      ],
      "metadata": {
        "id": "P159zdvdva_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}